\documentclass{article}
\usepackage[utf8]{inputenc}
\ProvidesPackage{lindrew}[2020/05/09] % with great amounts of help from Jason Chen

\newif\iflinserif \linseriffalse % by default, uses cmbright sans serif font.
\newif\iflinclass \linclasstrue % by default, numbers all theorems/definitions from 1 to N w/o regard for section
\newif\iflinindent \linindenttrue % by default, does do paragraph indent
\newif\iflinheader \linheadertrue % by default, no header

\DeclareOption{serif}{\linseriftrue}  % if you don't like the font
\DeclareOption{formal}{\linclassfalse} % if you want a document where theorems in section 1 are "theorem 1.x", etc.
\DeclareOption{noindent}{\linindentfalse}
\DeclareOption{header}{\linheadertrue}

\ProcessOptions*

\iflinserif\else
    \usepackage{cmbright}
\fi

\iflinindent
\else
    \setlength{\parindent}{0pt}
\fi

\iflinheader
    \usepackage[includehead,includefoot, heightrounded,  left=0.8in, top=1.6cm, bottom=2.4cm, right=0.8in]{geometry}
    \usepackage{fancyhdr}
    \pagestyle{fancy}
    \rhead{\textbf{Mauricio Barba da Costa}} % change name as necessary
\else
    \usepackage[left=0.8in, top=1.6cm, bottom=2.4cm, right=0.8in]{geometry}
\fi

\usepackage{amsmath, amssymb, amsthm} % standard
\usepackage[svgnames, dvipsnames, usenames]{xcolor}
\usepackage[colorlinks]{hyperref} % inserting links - copied from Jason
\PassOptionsToPackage{colorlinks}{hyperref}
\hypersetup{urlcolor = RedViolet, linkcolor = RoyalBlue, citecolor = ForestGreen}
\urlstyle{same}

\usepackage[capitalize]{cleveref} % for cross-referencing theorems, etc.
\usepackage{graphicx} % for putting pictures
\usepackage[T1]{fontenc} % for accents, etc.
\usepackage{verbatim} % for commenting out sections of text
\usepackage[nodisplayskipstretch, onehalfspacing]{setspace} % 1.5 spacing
\usepackage{tikz, tikz-cd, pgfplots} % for drawing pictures
\usepackage[style=numeric, sorting=none]{biblatex} % citations
\renewbibmacro{in:}{} % remove "In:" in the bibliography

\newcommand{\vocab}[1]{\textbf{\color{blue!90}\boldmath #1}}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\pnorm}[1]{\left|\left|#1\right|\right|}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}

% Fixes itemize
\usepackage{enumitem}
\setlist{itemsep = 0.2em, topsep = 0.4em}
\renewcommand\labelitemi{\raisebox{0.15em}{\tiny$\bullet$}}

% Changes size of section headers
\usepackage{titlesec}
\titleformat*{\section}{\LARGE\bfseries\sffamily}
\titleformat{\subsection}{\Large\bfseries\sffamily}{\thesubsection}{0.4cm}{}

% Blackboard bold
\newcommand{\Aff}{\mathbb{A}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Sphere}{\mathbb{S}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Qbar}{{\overline{\Q}}}
\newcommand{\Zhat}{{\widehat{\Z}}}
\newcommand{\Zbar}{{\overline{\Z}}}
\newcommand{\kbar}{{\overline{k}}}
\newcommand{\Kbar}{{\overline{K}}}
\newcommand{\Fbar}{{\overline{\F}}}

\newcommand{\boldmu}{\bm{\mu}}
\newcommand{\boldalpha}{\bm{\alpha}}

\newcommand{\ii}{\mathbf{i}}
\newcommand{\jj}{\mathbf{j}}
\newcommand{\kk}{\mathbf{k}}

\newcommand{\pp}{\mathfrak{p}}
\newcommand{\mm}{\mathfrak{m}}

% mathcal characters
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calJ}{\mathcal{J}}
\newcommand{\calK}{\mathcal{K}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calQ}{\mathcal{Q}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}

\newcommand{\CC}{\mathscr{C}}
\newcommand{\FF}{\mathscr{F}}
\newcommand{\GG}{\mathscr{G}}
\newcommand{\scriptH}{\mathscr{H}}
\newcommand{\II}{\mathscr{I}}
\newcommand{\JJ}{\mathscr{J}}
\newcommand{\KK}{\mathscr{K}}
\newcommand{\LL}{\mathscr{L}}
\newcommand{\OO}{\mathscr{O}}
\newcommand{\XX}{\mathscr{X}}
\newcommand{\ZZ}{\mathscr{Z}}

% various math expressions that shouldn't be italicized
\DeclareMathOperator{\cis}{cis}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator*{\lcm}{lcm}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\res}{res}
\DeclareMathOperator{\rad}{rad}
\DeclareMathOperator{\Spec}{Spec}

\newcommand{\Cech}{\v{C}ech}
\newcommand{\del}{\partial}
\newcommand{\directsum}{\oplus} % binary direct sum
\newcommand{\Directsum}{\bigoplus} % direct sum of a collection
\newcommand{\injects}{\hookrightarrow}
\newcommand{\intersect}{\cap} % binary intersection
\newcommand{\Intersection}{\bigcap} % intersection of a collection
\newcommand{\isom}{\simeq}
\newcommand{\HH}{{\operatorname{H}}}
\newcommand{\HHcech}{{\check{\HH}}}
\newcommand{\HHat}{{\hat{\HH}}}
\newcommand{\notdiv}{\nmid}
\newcommand{\surjects}{\twoheadrightarrow}
\newcommand{\tensor}{\otimes} % binary tensor product
\newcommand{\Tensor}{\bigotimes} % tensor product of a collection
\newcommand{\To}{\longrightarrow}
\newcommand{\union}{\cup} % binary union
\newcommand{\Union}{\bigcup} % union of a collection

% can't use DeclareMathOperator because already defined
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\renewcommand{\mod}[1]{\ \text{mod}\ #1}

% other convenient abbreviations
\newcommand{\eps}{\varepsilon}
\DeclareMathOperator{\indep}{\perp\!\!\!\perp}

% Mau's new commands
\usepackage{array}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\ra}[1][]{\xrightarrow{#1}}
\DeclareMathOperator{\inn}{inn}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\im}{im}

% theorem boxes
\usepackage{thmtools}
\usepackage[framemethod = TikZ]{mdframed}
\usepackage{silence} % for suppressing warnings
\WarningFilter{mdframed}{You got a bad break}

\mdfsetup{
	linewidth = 0.3mm,
	innertopmargin = 2mm,
	innerbottommargin = 3.5mm,
	innerleftmargin = 3mm,
	innerrightmargin = 3mm
} % adjusts boundaries of boxes

\newcommand{\thmboxstyle}[4]{
	\mdfdefinestyle{#2}{
		linecolor = #3,
		backgroundcolor = #4,
		nobreak = true
	}
	\declaretheoremstyle[
		headfont = \sffamily\bfseries\color{#3},
		mdframed = {style = #2},
		headpunct = {\\[0.4pt]},
		postheadspace = {0pt},
	]{#1}
}

% four different colors of boxes

\thmboxstyle{defbox}{mdredbox}{red}{orange!5}
\thmboxstyle{thmbox}{mdbluebox}{blue!90!red}{cyan!4}
\thmboxstyle{exbox}{mdgreenbox}{green!70!black}{teal!4}
\thmboxstyle{notebox}{mdorangebox}{orange!50!brown}{yellow!5!olive!5}

\iflinclass
    \declaretheorem[style = thmbox, name = Theorem]{theorem}
\else
    \declaretheorem[style = thmbox, name = Theorem, numberwithin = section]{theorem}
\fi

\declaretheorem[style = thmbox, name = Lemma, sibling = theorem]{lemma}
\declaretheorem[style = thmbox, name = Proposition, sibling = theorem]{proposition}
\declaretheorem[style = thmbox, name = Corollary, sibling = theorem]{corollary}
\declaretheorem[style = thmbox, name = Conjecture, sibling = theorem]{conjecture}

\declaretheorem[style = thmbox, name = Theorem, numbered = no]{theorem*}
\declaretheorem[style = thmbox, name = Lemma, numbered = no]{lemma*}
\declaretheorem[style = thmbox, name = Proposition, numbered = no]{proposition*}
\declaretheorem[style = thmbox, name = Corollary, numbered = no]{corollary*}
\declaretheorem[style = thmbox, name = Conjecture, numbered = no]{conjecture*}

\declaretheorem[style = defbox, name = Definition, sibling = theorem]{definition}
\declaretheorem[style = defbox, name = Definition, numbered = no]{definition*}

\declaretheorem[style = exbox, name = Example, sibling = theorem]{example}
\declaretheorem[style = exbox, name = Example, numbered = no]{example*}

\declaretheorem[style = notebox, name = Fact, sibling = theorem]{fact}
\declaretheorem[style = notebox, name = Fact, numbered = no]{fact*}
\declaretheorem[style = notebox, name = Problem, sibling = theorem]{problem}
\declaretheorem[style = notebox, name = Problem, numbered = no]{problem*}

\declaretheorem[style = plain, name = Question, sibling = theorem]{question}
\declaretheorem[style = plain, name = Question, numbered = no]{question*}

\declaretheorem[style = plain, name = Claim, sibling = theorem]{claim}
\declaretheorem[style = plain, name = Claim, numbered = no]{claim*}

\declaretheorem[style=plain, name=Remark, sibling=theorem]{remark}
\declaretheorem[style=plain, name=Remark, numbered=no]{remark*}

% Makes title bold
\makeatletter
\patchcmd{\@maketitle}{\LARGE}{\LARGE\bfseries}{}{}
\makeatother
\title{Analysis notes}
\author{Mauricio Barba da Costa}

\begin{document}

\maketitle
\section{September 10,2020}
\begin{definition}
A sequence $a_n$ is a map $f:\N-\R$. The sequence converges to $a$ if and only if for all $\forall\eps\exists N s.t. n>N\implies |a_n-a|<\eps$
\end{definition}
\begin{theorem}
If $a_n$ and $b_n$ are sequences that converge to $a$ and $b$ respectively then $a_nb_n$ converges to $ab$.
\end{theorem}
\begin{proof}
Given $\eps$ there exists $N_1$ such that $n>N_1\implies |a_n-a|<\frac{\eps}{2(|b|+1)}$. There exists $M$ such that for $n$ bigger than $M$, $|a_n|-|a|\leq|a-a_n|<1\implies|a_n|<|a|+1$. Given $\epsilon$ there exists $N_2$ such that $n>N_2\implies |b_n-b|<\frac{\epsilon}{2(|a|+1)}$. $|a_nb_n-ab|=|a_nb_n-a_nb+a_nb-ab|=|a_n(b_n-b)+b(a_n-a)|\leq|a_n||b_n-b|+|b||a_n-a|$.  Thus when $n>\max\{N_1,N_2,M\}$, $|a_n||b_n-b|+|b||a_n-a|<(|a|+1)|b_n-b|+|b||a_n-a|<\frac{\eps}{2}+\frac{\eps|b|}{2(|b|+1)}<\epsilon$.
\end{proof}
\begin{proposition}
If $a_n$ converges to $a$ then $\frac{1}{a_n}$ converges to $1/a$.
\end{proposition}
\begin{proof}
$|\frac{1}{a_n}-\frac{1}{a}|=|\frac{a-a_n}{a_na}|=\frac{|a-a_n|}{|a_n||a|}$. There exists $M$ such that for $n$ greater than $M$, $|a|-|a_n|\leq|a_n-a|\leq|a|/2\implies -|a_n|\leq-|a|/2\implies |a_n|\geq|a|/2$ and there exists $N$ such that for $n$ greater than $N$, $|a-a_n|<\eps|a|^2/2$. Thus if $n>\max\{N,M\}$ $\frac{|a-a_n|}{|a_n||a|}\leq\frac{2|a-a_n|}{|a|^2}<\eps$.
\end{proof}
\begin{theorem}[Monotone Convergence Theorem]
Any bounded monotone sequence is convergent
\end{theorem}
\begin{proof}
Assume that $a_n$ is monotone increasing. Let $a=\sup a_n$ since $a_n$ is assumed to be bounded. For all $\eps$ there exists $N$ such that $a_N>a-\epsilon$. Since the sequence is monotone, $n>N\implies a_n>a-\eps$
\end{proof}
\begin{theorem}[Squeeze theorem]
If $a_n\leq c_n\leq b_n$ for all $n$ and $a_n$ and $b_n$ converge to $a$ then $c_n$ converges to $a$.
\end{theorem}
\begin{proof}
Given $\eps$ there exists $N_1$ such that $n\geq N_1\implies |a_n-a|<\eps\implies a-\eps<a_n$. There exists $N_2\; s.t. \; n\geq N\implies |b_n-b|<\epsilon\implies b_n<a+\epsilon$. Let $n\geq \max\{N_1,N_2\}$. Then $a-\eps<a_n\leq c_n\leq b_n<a+\epsilon$. Thus $|c_n-a|<\eps$.
\end{proof}
\begin{definition}
A sequence $a_n$ is a Cauchy sequence if and only if given $\eps$ there exists $N$ such that for $n>N$ and $m>N$ $|a_n-a_m|<\epsilon$.
\end{definition}
\begin{theorem}[Cauchy's theorem]
Any Cauchy sequence is convergent.
\end{theorem}
\begin{definition}
A mapping $T:\R\xrightarrow[]{}\R$ is a contraction mapping if and only if there exists $0<\gamma<1$ such that $|T(x)-T(y)|\leq\gamma|x-y|$ for all $x,y\in\R$.
\end{definition}
\begin{definition}
A fixed point of $T:\R\ra\R$ is a point $x$ such that $T(x)=x$.
\end{definition}
\begin{theorem}[Contraction mapping theorem]
Any contraction mapping has a fixed point.
\end{theorem}
\begin{example}
The map $T(x)=\frac{1}{2}x+1$ is a contraction mapping since $|T(x)-T(y)|=|(\frac{1}{2}x+1)-(\frac{1}{2}y+1)|=|\frac{1}{2}x-\frac{1}{2}y|=\frac{1}{2}|x-y|$. It has a fixed point by the contraction mapping theorem. $T(x)=\frac{1}{2}x+1=x\implies \frac{1}{2}x=1\implies x=2$.
\end{example}
\begin{proof}[Proof of the contraction mapping theorem]
Let $T$ be a contraction mapping with $|T(x)-T(y)|\leq\gamma|x-y|$. Create a sequence inductively by $a_0=x_0$ where $x_0$ is some real number and $a_{k+1}=T(a_k)=T^n(x_0)$. $|a_{n+1}-a_n|=|T^{n+1}(x_0)-T^n(x_0)|=|T^n(T(x_0))-T^n(x_0)|=|T(T^{n-1}(T(x_0)))-T(T^{n-1}(x_0))|\leq\gamma|T^{n-1}(T(x_0))-T^{n-1}(x_0)|\leq\gamma^n|T(x_0)-x_0|$. For any arbitrary $m,n$ with $m\geq n, |a_m-a_n|\leq|a_m-a_{m-1}|+|a_{m-1}-a_{m-2}|+...|a_{n+1}-a_n|\leq(\gamma^{m-1}+\gamma^{m-2}+...\gamma^{n})|T(x_0)-x_0|=\frac{\gamma^n-\gamma^m}{1-\gamma}|T(x_0)-x_0|$. Given $\epsilon$ there exists $N$ such that $\frac{\gamma^{N}}{1-\gamma}<\frac{\epsilon}{|T(x_0)-x_0|}$. If $n$ and $m$ are greater than $N$ then $\frac{\eps}{|T(x_0)-x_0|}>\frac{\gamma^{N}}{1-\gamma}>\frac{\gamma^n}{1-\gamma}>\frac{\gamma^n-\gamma^m}{1-\gamma}$. Thus $a_n$ is Cauchy. $|T(a)-a|=|T(a)-T^{n+1}(x_0)+T^{n+1}(x_0)-T^n(x_0)+T^n(x_0)-a|\leq|T(a)-T^{n+1}(x_0)|+|T^{n+1}(x_0)-T^n(x_0)|+|T^n(x_0)-a|<2|a-T^n(x_0)|+|T^{n+1}(x_0)-T^n(x_0)|\leq 2|a-T^n(x_0)|+\gamma^n|T(x_0)-x_0|$. Since $a_n$ converges to $a$, given $\epsilon$ there exists $N$ such that $n>N\implies |a-T^n(x_0)|<\epsilon/3$ and since $\gamma^n$ converges to 0, given $\epsilon$, there exists $M$ such that $n>M\implies \gamma^n|T(x_0)-x_0|<\epsilon/3$. Thus if $n>\max\{N,M\}$, $|T(a)-a|<\epsilon$. Since $\epsilon$ can be made arbitrarily small, $T(a)=a$.
\end{proof}
\section{September 15, 2020}
\begin{fact}
Cauchy sequences are useful because you can say a sequence converges without identifying the point it converges to.
\end{fact}
\begin{proof}
Assume first that $a_n$ is convergent. Then given $\eps$ there exists $N$ such that $n>N\implies |a_n-L|<\eps/2$. Let $m>N$ then $|a_n-a_m|\leq|a_n-L|+|L-a_m|<\eps$.\\
Conversely, assume that $a_n$ is a Cauchy sequence. First observe that a Cauchy sequence is bounded. If we set $\eps=1$ we can find $N$ such tha $n,m\geq N\implies |a_n-a_m|<1$. In particular, $|a_n-a_N|<1$ for all but $a_1,...,a_{N-1}$. Thus $a_n$ is bounded by $\max\{a_1,...,a_{N-1},a_N+1\}$ and $\min\{a_1,...,a_{N-1},a_N-1\}$ which are finite. Define $b_n:=\sup_{i\geq n}a_i$. Such a $\sup$ exists since $a_n$ is bounded. $b_{n+1}=\sup_{i\geq n+1}a_i\geq\sup_{i\geq n}a_i=b_n$. The $b_n$ sequence is thus monotone so it converges to some number $a$. Given $\epsilon>0$ there exists a big enough $N$ such that simultaneously if $n,m\geq N$ then $|a_n-a_m|<\epsilon/3$ and if $n\geq N$ then $|a-b_n|<\epsilon/3$. From the definition of $b_n$, there exists $i\geq n$ such that $|b_n-a_i|<\eps/3$. Thus $|a-a_n|\leq|a-b_n|+|b_n-a_i|+|a_i-a_n|<\eps$.
\end{proof}
We showed before that a contraction mapping has a fixed pint.
\begin{proposition}
A contraction mapping has at most one fixed point
\end{proposition}
\begin{proof}
Suppose $y_1$ and $y_2$ are fixed points. Since $y_1$ and $y_2$ are fixed points, $|T(y_1)-T(y_2)|=|y_1-y_2|$ and since $T$ is a contraction mapping, $|T(y_1)-T(y_2)|\leq\gamma|y_1-y_2|<|y_1-y_2|$.
\end{proof}
\begin{theorem}[Bolzano-Weierstrass Theorem]
Any bounded sequence has a convergent subsequence.
\end{theorem}
\begin{proof}
    Let $p_n$ be a bounded sequence.
\begin{enumerate}
    \item[Case 1:]  For every subsequence $q_n$ of $p_n$, there exists $i$ such that given $N$ there exists $n\geq N$ such that $q_n>q_i$.

    Then there exists $i$ such that given $N$ there exists $n\geq N$ such that $p_i<p_n$. Then let $\alpha_1=p_i$ and define $q_1=p_i$ and $q_k$ the $k-1$th term of $p_n$ that is strictly greater than $p_i$. Now, there exists $j$ such that given $N$ there exists $n\geq N$ such that $q_k<q_n$. Then let $\alpha_2=q_j$. Continue in this manner to get a strictly increasing subsequence of $p_n$.
    \item[Case 2:] There exists a sequence $q_n$ of $p_n$ such that given $i$ there exists $N$ such that $n\geq N\implies q_n\leq q_i$.
    Let $d_1=q_1$. There exists $i$ with $q_i\leq q_1$. Let $d_2=q_i$. Continue in this way to get a decreasing subsequence $d_i$ of $p_n$.

\end{enumerate}
\end{proof}
\begin{definition}
A series is a sequence of the form $$s_n=\sum_{i=1}^n{a_i}$$
\end{definition}
\begin{definition}[Geometric series]
Given $0<\gamma<1$, let $a_n=\gamma^n$. Then $s_n=\sum_{i=0}^n\gamma^i=\frac{1-\gamma^{n+1}}{1-\gamma}$. Then $\lim_{n\ra\infty}s_n=\frac{1}{1-\gamma}$. If instead you had started at $i=a$ then you get $\sum_{i=a}^n\gamma^i=\frac{\gamma^a}{1-\gamma}$.
\end{definition}
\begin{definition}[Absolute convergence]
A series $\sum_{i=1}^n a_i$ converges absolutely if and only if $\sum_{i=1}^n|a_i|$ converges.
\end{definition}
\begin{proposition}
If $\overline{s}_n=\sum_{i=1}^n|a_i|$ is convergent then $s_n=\sum_{i=1}^n a_i$ is also convergent. The converse is not necessarily true.
\end{proposition}
\begin{proof}
If $\overline{s}_n=\sum_{i=1}^n|a_i|$ is convergent then this is equivalent to saying that $\overline{s}_n$ is a Cauchy sequence. Let $n>m$. Then $|s_n-s_m|\leq|a_{m+1}|+...+|a_n|=|\overline{s}_n-\overline{s}_m|$. Thus if $\overline{s}_n$ is Cauchy (convergent) then $s_n$ is Cauchy (convergent)
\end{proof}
\section{September 17, 2020}
\subsection{Tests for convergence of series}
\begin{definition}[Comparison Test]
$$\sum_{n=1}^\infty a_n\quad \sum_{n=1}^\infty b_n$$
Suppose $0\leq a_n\leq b_n$ for all $n$ If $b_n$ converges then $a_n$ converges.
\end{definition}
\begin{proof}
Since $a_n\geq 0$, $s_N=\sum_{i=1}^Na_i$ is monotone increasing. It is bounded above by $\sum_{n=i}^\infty b_i$. By the monotone convergence theorem, $s_N$ converges.
\end{proof}
\begin{definition}[Ratio Test]
$$\sum_{n=1}^\infty a_n$$
If $\lim\sup_{n\ra\infty}|\frac{a_{n+1}}{a_n}|<1$ then the sequence converges.
\end{definition}
\begin{proof}
If $\alpha=\limsup|\frac{a_{n+1}}{a_n}|<1$, then we can find $\alpha<\beta<1$ and an integer $N$ such that $|\frac{a_{n+1}}{a_n}|<\beta$ for $n\geq N$. In particular, $|a_{N+1}|<\beta|a_N|, |a_{N+2}|<\beta|a_{N+1}|<\beta^2|a_n|,|a_{N+p}|<\beta^p|a_N|$. Thus $|a_n|<|a_N|\beta^{-N}\beta^n$ for $n>N$ and the proof the claim follows from the comparison test and absolute convergence test since $\sum\beta^n$ converges.
\end{proof}
\begin{definition}[Alternating Series Test]
Suppose $\sum_{n=0}^\infty (-1)^n a_n$ where $a_n$ is monotone. Then the series converges if and only if $\lim_{n\ra\infty}a_n=0$.
\end{definition}
\begin{example}
We are going to see if $\sum_{n=0}^\infty n2^{-n}$ converges. Do the ratio test. $\frac{|a_{n+1}|}{|a_n|}=\frac{(n+1)2^{-(n+1)}}{n2^{-n}}=\frac{n+1}{n}\frac{1}{2}$ which converges to $\frac{1}{2}$ which is less than 1.
\end{example}
\begin{example}
$\sum_{n=0}^\infty \frac{2^n}{n!}$. The ratio test says that $\frac{|a_{n+1}|}{|a_n|}=\frac{2^{n+1}}{(n+1)!}\frac{n!}{2^n}=2\frac{n!}{(n+1)!}=2\frac{n!}{(n+1)n!}=\frac{2}{n+1}$ which converges to zero as $n$ goes to infinity.
\end{example}
\begin{example}
$(n!)^{1/n}=(n(n-1)(n-2)...2\cdot 1)^{1/n}$. Note that $n!\geq n(n-1)(n-2)...\frac{n}{2}\geq (\frac{n}{2})^{\frac{n}{2}}$. Thus $(n!)^{1/n}\geq ((\frac{n}{2})^\frac{n}{2})^\frac{1}{n}=(\frac{n}{2})^\frac{1}{2}$.
\end{example}
\begin{definition}[Root Test]
Suppose we have $\sum_{n=0}^\infty a_n$. If $\lim\sup_{n\ra\infty}\sqrt[n]{|a_n|}<1$ then the sum converges.
\end{definition}
\begin{proof}
Let $\alpha=\lim\sup_{n\ra\infty}\sqrt[n]{|a_n|}$. If $\alpha<1$, we can choose $\beta$ such that $\alpha<\beta<1$ and an integer $N$ such that $n>N\implies \sqrt[n]{|a_n|}<\beta \iff |a_n|<\beta^n$. Since $0<\beta<1$, $\sum \beta^n$ converges. Convergence of $\sum a_n$ follows from the comparison test.
\end{proof}
\begin{example}
Consider $$\sum_{n=0}^\infty \frac{x^n}{n!}$$ and we ask ourselves for which $x$ does this converge? $$\frac{\frac{x^{n+1}}{(n+1)!}}{\frac{x^n}{n!}}=\frac{x^{n+1}n!}{(n+1)!x^n}=\frac{x}{n+1}$$. By the ratio test, the sum is convergent for all $x$.
\end{example}
\begin{example}
Look at $$\sum_{n=0}^\infty x^n.$$ We already know that this is convergent if and only if $|x|<1$. The ratio test and root test output identical answers, that $|\frac{x^{n+1}}{x^n}|=|x|<1$. We can think of 1 as being the "radius of convergence of the sum.
\end{example}
\begin{definition}[A continuous function]
A function $f:I\ra\R$ (where I is an interval) is continuous at $x_0\in I$ if and only if for all $\eps>0$ there exists $\delta>0$ such that $|x-x_0|<\delta\implies|f(x)-f(x_0)|<\eps$. $f$ is continuous on $I$ if it is continuous at all $x_0\in I$.
\end{definition}
\begin{proposition}
Let $h$ and $f$ be continuous functions. Then $h(x)+f(x)$ is also a continuous function.
\end{proposition}
\begin{proof}
Given $x_0\in I$ and $\eps>0$, pick $\delta$ such that simultaneously $|x-x_0|<\delta\implies |f(x)-f(x_0)<\epsilon/2$ and $|x-x_0|<\delta\implies |h(x)-h(x_0)|\eps/2$. Then if $|x-x_0|<\delta$, $|f(x)+h(x)-(f(x_0)+h(x_0))|\leq|f(x)-f(x_0)|+|h(x)-h(x_0)|<\eps$.
\end{proof}
\begin{proposition}
If $f$ and $g$ are continuous functions then $fg$ is continuous.
\end{proposition}
\begin{proof}
Let $h(x)=f(x)g(x)$. Given $\eps$ and $x_0$ there exists $\delta$ such that simultaneously $|f(x)-f(x_0)|<\frac{\eps}{2(|g(x_0)|+1)}$ and $|g(x)-g(x_0)|<\frac{\eps}{2(|f(x_0)|+1)}$ and $|g(x)|-|g(x_0)|\leq|g(x_0)-g(x)|<1$. Then $|h(x)-h(x_0)|=|f(x)g(x)-f(x_0)g(x_0)|=|f(x)g(x)-f(x_0)g(x)+f(x_0)g(x)-f(x_0)g(x_0)|\leq|g(x)||f(x)-f(x_0)|+|f(x_0)||g(x)-g(x_0)|<(|g(x_0)|+1)|f(x)-f(x_0)|+|f(x_0)||g(x)-g(x_0)|<\frac{\eps}{2}+\frac{\eps|f(x_0)|}{2(|f(x_0)|+1)}<\eps$.
\end{proof}
\section{September 24, 2020}
Last time we showed that if $f$ and $g$ are continuous then $fg$ is continuous. A corollary to this is that if $f$ is continuous and it not 0 on an interval then $\frac{1}{f}$ is continuous on that interval.
\begin{example}
A constant function is continuous.
\end{example}
\begin{example}
Since we know that constant functions and $x$ are continuous, all polynomials are continuous.
\end{example}
\begin{theorem}
Let $f$ and $g$ be continuous functions that map $\R$ to $\R$ and assume that $f(x)=g(x)$ for all $x\in \Q$. Then $f(x)=g(x)$ for all $x\in \R$. In other words $f|_\Q$ uniquely defines $f$ over the reals. In general $f$ restricted to any dense subset of $\R$ uniquely defines $f$.
\end{theorem}
\begin{proof}
Given $\eps$ and $x_0$, there exists $\delta$ such that simultaneously $|x-x_0|<\delta\implies |f(x)-f(x_0)|<\eps/2$ and $|g(x)-g(x_0)|<\eps/2$. Then $|f(x_0)-g(x_0)|\leq|f(x_0)-f(x)|+|f(x)-g(x)|+|g(x)-g(x_0)|$. For all $\delta>0$ there exists $x\in \Q$ such that $|x_0-x|<\delta$. Thus $|f(x_0)-g(x_0)|\leq|f(x_0)-f(x)|+|f(x)-g(x)|+|g(x)-g(x_0)|< \eps/2+0+\eps/2=\eps$. Since $\eps$ can be made arbitrarily small, $f(x_0)=g(x_0)$ for all $x_0\in \R$.
\end{proof}
\begin{definition}[Power Series]
Let's go back to the power series. Let $\{a_n\}$ be a sequence and $\sum_{n=0}^\infty a_n x^n$ is a power series.
\end{definition}
\begin{example}
Last time we used the ratio test to show that $E(x)=\sum_{n=0}^\infty\frac{x^n}{n!}$ converges for all $x\in \R$. This function is also continuous. $E(0)=1$ and $E(1)=e$
\end{example}
\begin{proposition}
$E(x+y)=E(x)E(y)$
\end{proposition}
\begin{corollary}
Let $n,p,q\in\Z$
\begin{itemize}
\item $E(n)=E(\sum^n 1)=E(1)^n=e^n$.
\item $E(\frac{1}{n})=\sqrt[n]{e}$ since $e=E(\frac{1}{n})^n$.
\item $E(\frac{p}{q})=\sqrt[q]{e^p}=e^\frac{p}{q}$
\item $E(x)=e^x$ since they are the same on the rationals. (we just need to prove that $E(x+y)=E(x)E(y)$ and $E(x)$ is continuous).
\end{itemize}
\end{corollary}
\begin{proof}[Proof of proposition 37]
Let $E(x)=\sum_{n=0}^\infty\frac{x^i}{i!}$ and $E(y)=\sum_{n=0}^\infty\frac{y^i}{i!}$.

Observe that $(x+y)^n=\sum_{i=0}^n{n\choose i}x^iy^{n-i}$.

$\sum_{i+j\leq N}\frac{x^i}{i!}\frac{y^j}{j!}=\sum_{n=0}^N \frac{1}{n!}\sum_{i=0}^n{n\choose i}x^iy^{n-i}=\sum_{n=0}^N\frac{(x+y)^n}{n!}$ so $\sum_{n=0}^N \frac{1}{n!}\sum_{i=0}^n{n\choose i}x^iy^{n-i}$ converges to $E(x+y)$.

Observe that $$\sum_{i+j\leq N} \frac{x^iy^j}{i!j!}<\sum_{i=1}^N\frac{x^i}{i!}\sum_{j=1}^N\frac{y^j}{j!}<\sum_{i+j\leq 2N}\frac{x^iy^j}{i!j!}$$
As $N$ goes to infinity, the middle term is $E(x)E(y)$ and the left and right terms converge to $E(x+y)$. By the squeeze theorem $E(x)E(y)=E(x+y)$.
\end{proof}
\subsection{Tests of convergence for series}
\begin{tabular}{|c|c|}
\hline
Comparison & Suppose $0\leq a_n\leq b_n$ for all $n$. If $b_n$ converges then $a_n$ converges. \\
\hline
Ratio & If $\limsup_{n\ra\infty}|\frac{a_{n+1}}{a_n}|<1$ then the sum converges.\\
\hline
Root & If $\limsup_{n\ra\infty}\sqrt[n]{|a_n|}<1$ then the sum converges.\\
\hline
Alternating Series Test & If $a_{n+1}a_n<0,|a_{n+1}|\leq|a_n|$, and $\lim_{n\ra\infty}a_n=0$ then the sum converges. \\
\hline
\end{tabular}
\begin{example}
$\sum_{n=1}^\infty \frac{(-1)^n}{n}$ is an alternating series. Sign of $a_n=\frac{(-1)^n}{n}$ changes, $|a_{n+1}|\leq |a_n|$, and $|a_n|$ tends to zero.
\end{example}
\begin{definition}[Alternating Series Test]
If the following hold:
\begin{itemize}
\item $a_{n+1}a_n<0$
\item $|a_{n+1}|\leq |a_n|$
\item $\lim_{n\ra\infty}a_n=0$
then the alternating series test implies that this series converges.
\end{itemize}
\end{definition}
\begin{proof}
$S_{2n+2}=S_{2n}+a_{2n+1}+a_{2n+2}$ so $S_{2n+2}\leq S_{2n}$ so $S_{2n}$ is monotone decreasing. A similar argument shows $S_{2n+1}$ is monotone increasing. $S_{2n+1}\leq S_{2n}$ and $S_{2n+1}-S_{2n}=a_{2n+1}$ which goes to zero. By the squeeze theorem, the claim follows.
\end{proof}
\section{September 29, 2020}
\begin{remark}
It might be advantageous to fix the proof for the previous problem and take notes on the rearrangement of series.
\end{remark}
\begin{proposition}
If $f:I\ra \R$ is continuous and $x_n\ra x$, $x_n,x\in I$, then $f(x_n)\ra f(x)$
\end{proposition}
\begin{proof}
Given $\eps$ and $x$ there exists $\delta$ such that $|x'-x|<\delta\implies |f(x')-f(x)|<\eps$. Given $\delta$ there exists $N$ such that $n>N\implies |x_n-x|<\delta$ ao that $|f(x_n)-f(x)|<\eps$.
\end{proof}
\begin{theorem}[The Extreme Value Theorem]
If $f:[a,b]\ra \R$ is continuous then there exists $M\in I$ such that $f(M)=\sup\Im f$ and there exists $m$ such that $f(m)=\inf\Im f$.
\end{theorem}
\begin{theorem}[Intermediate Value Theorem]
Let $f:[a,b]\ra \R$ be continuous. If $f(a)\leq y\leq f(b)$ then there exists $x\in [a,b]$ such that $f(x)=y$.
\end{theorem}
\begin{proof}[Proof of the extreme value theorem]
Suppose the function $f$ is not bounded above on the interval $[a,b]$. Then for every natural number $n$, there exists an $x_n\in [a,b]$ such that $f(x_n)>n$. This defines a sequence $(x_n)_{n\in\N}$. Because $[a,b]$ is bounded, the Bolzano Weierstrass theorem implies that there is a convergent subsequence $(x_{n_k})_{k\in\N}$ of $(x_n)$. Denote its limit by $x$. As $[a,b]$ is closed, it contains $x$. Because $f$ is continuous at $x$, we know that $f(x_{n_k})$ converges to the real number $f(x)$. But $f(x_{n_k})>n_k\geq k$ for every $k$, which implies that $f(x_{n_k})$ diverges to $\infty$, a contradiction. Therefore, $f$ is bounded on $[a,b]$.

$\sup\im f$ exists. It is necessary to find $d\in [a,b]$ such that $M=f(d)$. Let $n$ be a natural number. As $M$ is the least upper bound $M-\frac{1}{n}$ is not an upper bound for $f$. Therefore, there exists $d_n$ in $[a-b]$ so that $M-\frac{1}{n}<f(d_n)$. This defines a sequence $\{d_n\}$. Since $M$ is an upper bound for $f$, we have $M-\frac{1}{n}<f(d_n)\leq M$ for all $n$> Therefore, the sequence $\{f(d_n)\}$ converges to $M$. By the Bolzano-Weiestrass theorem, there exists a subsequence $\{d_{n_k}\}$, which converges to some $d$ and, as $[a,b]$ is closed $d$ is in $[a,b]$. Since $f$ is continuous at $d$, the sequence $\{f(d_{n_k})\}$ converges to $f(d)$. But $\{f(d_{n_k})\}$ is a subsequence of $\{f(d_n)\}$ that converges to $M$ so $M=f(d)$. Therefore, $f$ attains its supremum $M$ at $d$.
\end{proof}
\begin{proof}[Proof of the intermediate value theorem]
First, let $f(a)<u<f(b)$. Let $S$ be the set of all $x\in[a,b]$ such that $f(x)\leq u$. Then $S$ is non-empty since $a$ is an element of $S$ and $S$ is bounded above by $b$. $c=\sup S$ exists. We claim that $f(c)=u$. Given $\eps>0$ there exists $\delta>0$ such that $|f(x)-f(c)|<\eps$ whenever $|x-c|<\delta$. Thus $f(x)-\eps<f(c)<f(x)+\eps$ for all $x\in (c-\delta,c+\delta)$. There exists some $a^*\in (c-\delta,c]$ that is contained in $S$ so $f(c)<f(a^*)+\eps\leq u+\eps$. Picking $a^{**}\in(c,c+\delta)$, we knkow that $a^{**}\notin S$ because $c$ is the supremums of $S$. This means that $f(c)>f(a^{**}-\eps>u-\eps$. Both  inequalities mean $u-\eps<f(c)<u+\eps$. Thus $f(c)=u$.
\end{proof}
In both of these, it is crucial that $[a,b]$ be closed sets. The interval $(0,1]$ doesn't satisfy the extreme value theorem for $f:(0,1]\ra\infty$, $x\mapsto \frac{1}{x}$ since the function is unbounded as you get closer to zero. If you take an unconnected set, then the intermediate value theorem again doesn't work.
\begin{definition}[A metric space]
A metric space is a set $X$ endowed with a matric $d:X\times X\ra \R$ such that
\begin{itemize}
    \item $d(x_1,x_2)\geq 0$ with equality if and only if $x_1=x_2$.
    \item $d(x_1,x_2)=d(x_2,x_1)$
    \item $d(x_1,x_3)\leq d(x_1,x_2)+d(x_2,x_3)$
\end{itemize}
\end{definition}
\begin{example}
\begin{itemize}
    \item $\R$ endowed with the absolute value metric $d(x_1,x_2)=|x_1-x^2|$.
    \item $\R^2$ with $d((x_1,y_1),(x_2,y_2))=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$
    \item $\R^2$ with $d((x_1,y_1),(x_2,y_2))=\max\{|x_1-x_2|,|y_1-y_2|\}$.
    \item $C([0,1],\R)$ is the set of continuous functions from $[0,1]$ to $\R$. For \\$f,g\in C([0,1],\R), d(f,g)=\sup_{x\in[0,1]}|f(x)-g(x)|$
    \item French Railroad metric. The shortest path between two cities via railway. The set here is the earth and the metric is the shortest time it takes to go from one city to another via railway.
\end{itemize}
\end{example}
\section{September 1, 2020}
\begin{definition}[Convergene in a metric space]
A sequence $x_n$ converges to $x$ if and only if given $\eps$ there exists $N$ such that $n\geq N\implies d(x_n,x)<\eps$.
\end{definition}
\begin{proposition}
The limit of a convergent subsequence is unique..
\end{proposition}
\begin{proof}
Suppose $x_n$ converges to $x$ and $y$. Given $\eps$ there exists $N$ such that $n\geq N\implies d(x_n,x)<\eps/2$ and $M$ such that $n\geq M\implies d(x_n,y)<\eps/2$. By the triangle inequality, $d(x,y)<d(x_n,x)+d(x_n,y)<\eps$ so $x=y$.
\end{proof}
\begin{definition}[Cauchy Sequence in Metric Space]
Let $(X,d)$ be a metric space and $x_n$ a sequence. $x_n$ is Cauchy if and only if given $\eps$ there exists $N$ such that $n,m\geq N\implies d(x_n,x_m)<\eps$
\end{definition}
\begin{example}
If we have, $\R^n$ and $x_n$ a Cauchy Sequence. $|x-y|=d(x,y)$ then we showed that $x_n$ converges
\end{example}
\begin{example}
In $\Q$ with metric $d(x,y)=|x-y|$, the sequence $x_n$ that tends to $\sqrt{2}$ is Cauchy (since it converges in $\R$) but it does not converge over $\Q$.
\end{example}
\begin{example}
The sequence $x_n=\frac{1}{n}$ does not converge in $(0,1)$. The moral of the story is that the cauchy sequence, convergent sequence equivalence is not true for all metric spaces.
\end{example}
\begin{definition}
A metric space $(X,d)$ is Cauchy complete if and only if every Cauchy sequence is convergent in that metric space.
\end{definition}
\begin{example}[Set Operations]
$\cap A_\alpha=\{x\in X:x\in A_\alpha$ for all $\alpha\}$. $\cup A_\alpha=\{x\in X:\exists \alpha$ s.t. $x\in A_\alpha\}$.
\end{example}
\begin{lemma}
$$X-(\bigcap_\alpha A_\alpha)=\bigcup_\alpha(X-A_\alpha)$$
\end{lemma}
\begin{definition}[Ball]
The ball of radius $r$ with center at $x$ is $B_r(x)=\{y\in X:d(x,y)<r\}$.
\end{definition}
\begin{definition}
In $\R^2$ with the Euclidean metric, the balls are circles without the border.
\end{definition}
\begin{definition}[Open set]
A subset $A\subseteq X$ is open if for all $x\in A$ there exists $r>0$ with $B_r(x)\subseteq A$. Observe that the empty set is open by vacuity
\end{definition}
\begin{proposition}
Let $A_1$ and $A_2$ be open subsets of a metric space $X$. The set $A_1\cup A_2$ is also open. The intersection of $A_1$ and $A_2$ is also open. By induction we can then show that the finite union or intersection of open sets is again open.
\end{proposition}
\begin{proof}
Let $x\in A_1\cap A_2$ then $x\in A_1$ hence $r_1>0$ $B_{r_1}(x)\subseteq A_1$ and $x\in A_2$ hence $r_2>0 B_{r_2}(x)\subseteq A_2$. Let $r=\min\{r_1,r_2\}>0$. Thus $B_r(x)\subseteq A_1\cap A_2$.
\end{proof}
\begin{example}
The intersection of infinitely many open sets is not necessarily open. For example $\cap_n(-\frac{1}{n},\frac{1}{n})=\{0\}$. The $\min$ that we used to prove that the intersection of open sets is open is what screws things up.
\end{example}
\begin{proof}[Proof That the Union of Infinitely Many Open Sets is Open]
$x\in \cup_\alpha A_\alpha$ then there exists $\alpha$ such that $x\in A_\alpha$. Since $A_\alpha$ is open, there exists a $r>0$ with $B_r(x)\subseteq A_\alpha\subseteq \cup_\alpha A_\alpha$.
\end{proof}
\begin{definition}[Closed Set]
A set $A$ is closed in a metric space $X$ if and only if $X-A$ is open.
\end{definition}
\section{October 6, 2020}
\begin{proposition}
The infinite intersection of closed sets is again closed since $X-\cap_\alpha A_\alpha=\cup_\alpha(X-\alpha_\alpha)$ and the infinite union of open sets is open. Again, the infinite union of closed sets is not necessarily closed. For example $$\bigcup_{0\leq a <1} [0,a]=[0,1)$$
\end{proposition}
\begin{proposition}
If a set $A$ is closed then for a sequence $\{x_n\}$ in $A$, the limit of the sequence as $n$ approaches infinity is contained in $A$.
\end{proposition}
\begin{proof}
If not, there exists $x\in X-A$. $X-A$ is an open set so there exists $r>0$ such that $x\in B_r(x)\subset X-A$. On the other hand, $x_n\ra x$ so $d(x,x_n)\ra 0$. Therefore, for $N$ sufficiently large, $x_n\in B_r(x)$ but this is a contradiction since $x_n\in A$.
\end{proof}
\begin{proposition}
The converse of Proposition 62 is also true
\end{proposition}
\begin{proof}
Let $A\subset X$. We know that if $x_n\in A$ and $x_n\ra x$, then $x\in A$. Our goal is to show that $A$ is closed. If $X-A$ is not open, then there exists $x\in X-A$ such that for all $r>0$, $B_r(x)\cap A\neq\emptyset$. But $x_n\in B_{\frac{1}{n}}\cap A\neq \emptyset$ and $d(x,x_n)<\frac{1}{n}\implies x_n\ra x$. Since $x_n$ converges to $x$, this forces $x$ to be in $A$, a contradiction.
\end{proof}
\begin{definition}
A \vocab{cover} of a set $A$ is a collection of sets $\{B_\alpha\}$ (open, closed, or neither) such that $A\subset \cup_\alpha B_\alpha$
\end{definition}
\begin{definition}
A set $A\subset X$ is said to be compact if any cover of $X$ by open subsets has a finite subcover
\end{definition}
\begin{example}
Consider the set $(-1,1)$. $\cup_n(-1+\frac{1}{n},1-\frac{1}{n})=(-1,1)$. The union of these sets is $(-1,1)$ yet no finite union of the above union contains $(-1,1)$.
\end{example}
\begin{example}
Any set of the form $[a,b]\subset\R$ is compact.
\end{example}
\begin{example}
In $\R^2$, under the Euclidean metric, the set $\overline{B_r(x)}=\{y\in \R:d(x,y)\leq r\}$ is compact.
\end{example}
\begin{proposition}
The set $\overline{B_r(x)}=\{y\in X:d(x,y)\leq r\}$
\end{proposition}
\begin{proof}
Let $X-\overline{B_r(x)}$. Then $d=d(z,x)>r$. Let $s=d-r$. $B_s(z)\cap \overline{B_r(x)}=\emptyset$. Assume not. There exists $u\in B_s(z)\cap \overline{B_r(x)}$ so $d(z,u)<s$ and $d(x,u)\leq r$. By the triangle inequality, $d(x,z)\leq d(x,u)+d(u,z)<r+s=d$
\end{proof}
\begin{definition}
A set $A\subset X$ is said to be bounded if there exists $0<r$ and $x\in X$ such that $A\subset B_r(x)$ for some $x$.
\end{definition}
\begin{lemma}
If $K$ is compact, then $K$ is closed.
\end{lemma}
\begin{proof}
Pick a point $p\notin K$. If $q\in K$, let $V_q$ and $W_q$ be open balls around $p$ and $q$ of radius $\frac{1}{2}d(p,q)$. Observe that if $x\in W_q$ then $d(p,q)\leq d(q,x)+d(x,p)<\frac{1}{2}d(p,q)+d(x,p)$ so $d(x,p)>\frac{1}{2}d(p,q)$ that is, all the points in this ball are at least $\frac{1}{2}d(p,q)$ from $p$. By compactness, a finite number of these balls, $W_{q1},...,W_{qN}$ cover $K$. Look at the corresponding balls $V_{q1},...,V_{qN}$. They are all centered at $p$. The smallest (their intersection) is a neighborhood of $p$ that contains no point of $K$. This shows that $K^c$ is open.
\end{proof}
Observe that if $X$ is an compact set, then $X\subset\cup_{x\in X}B_\eps(x)$. But then there exists a finite set $S$ such that $A\subset\cup_{x\in S}B_\eps(x)$
\section{October 8, 2020}
Last time we looked at the definition of a compact space.
\begin{theorem}
Let $(X,d)$ be a compact set and $x_n$ a sequence. There exists a subsequence $x_{n_k}$ such that $x_{n_k}$ is a Cauchy sequence.
\end{theorem}
\begin{proof}
Cover $X$ by balls or radius 1. Then $X\subset \cup_{x\in X} B_1(x)$. Compactness tells us that finitely many are needed to cover $X$. Call these balls $B_1(y_i)$. There exists a ball $B_1(y)$ with infinitely many $x_n$ contained in it. $X$ can be covered by finitely many $B_\frac{1}{2}(z_i)$ so so can $B_1(y)$ and so $B_1(y)=\cup_i (B_1(y)\cap B_\frac{1}{2}(z_i))$. At least one of $B_1(y)\cap B_\frac{1}{2}(z)$ must have infinitely many points of $x_n$. Continue in this way with $B_\frac{1}{4}, B_\frac{1}{8}, B_\frac{1}{16}, B_\frac{1}{32},...$. Our subsequence is going to be the $x_n$s contained in all the balls. Given $\eps>0$, choose $N$ such that $2^{-N+2}<\eps$. Then $d(x_n,x_m)<\eps$\\
\end{proof}
\begin{lemma}
Let $(X,d)$ be compact and $A_\alpha$ a family of closed subsets. Then $\cap_\alpha A_\alpha= \emptyset\implies$ there exists finitely many $A_1\cap ...\cap A_m=\emptyset$.
\end{lemma}
\begin{proof}
Define $O_\alpha=X-A_\alpha$ so $O_\alpha$ is open. The union of $\cup O_\alpha=X-\cap_\alpha A_\alpha=X$. By compactness, finitely many $O_\alpha$ cover $X$ so $X=O_1\cup...\cup O_m=X-\cap_i A_i$.
\end{proof}
\begin{theorem}
Let $(X,d)$ be a compact metric space. If $x_n$ is a sequence in $X$, then there is a convergent subsequence.
\end{theorem}
\begin{proof}
Let $(F_n)$ be a decreasing sequence of closed nonempty subsets of $X$ and let $G_n=X-F_n$. If $\cup_{n=1}^\infty G_n=X$ then $\{G_n:n\in \N\}$ is an open cover of $X$ so it has a finite subcover $\{G_{n_k}:k=1,2,...K\}$ since $X$ is compact. Let $$N=\max\{n_k:k=1,2,...,K\}$$ Then $\cup^N_{n=1}G_n=X$ so $$F_n=\bigcap_{n=1}^NF_n=X-\bigcup_{n=1}^NG_n=\emptyset$$ contrary to our assumption that every $F_n$ is nonempty. It follows that $\cup_{n=1}^\infty G_n\neq X$ and then $$\bigcap_{n=1}^\infty F_n=X-\bigcup_{n=1}^\infty G_n\neq\emptyset$$ meaning that $X$ has the finite intersection property for closed sets, so $X$ is sequentially compact. The result follows from the following lemma.
\end{proof}
\begin{definition}
A metric space has the finite intersection property for closed sets if every decreasing sequence of closed, nonempty sets has nonempty intersection.
\end{definition}
\begin{lemma}
If a metric space has the finite intersection property for closed sets then it is sequentially compact.
\end{lemma}
\begin{proof}
Suppose that $X$ has the finite intersection property. Let $(x_n)$ be a sequence in $X$ and define $$F_n=\overline{T_n},\quad T_n=\{x_k:k>n\}$$ Then $(F_n)$ is a decreasing sequence of non-empty closed sets so there exists $$x\in \bigcap_{n=1}^\infty F_n$$ Choose a subsequence $(x_{n_k})$ of $(x_n)$ as follows. For $k=1$, there exists $x_{n_1}\in T_1$ such that $d(x_{n_1},x)<1$, since $x\in F_1$ and and $T_1$ is dense in $F_1$. Similarly, since $x\in F_{n_1}$ and $T_{n_1}$ is dense in $F_{n_1}$, there exists $x_{n_2}\in T_{n_1}$ with $n_2>n_1$ such that $d(x_{n_2},x)<\frac{1}{2}.$ Continuing in this way (or by induction), given $x_{n_k}$ we choose $x_{n_{k+1}}\in T_{n_k}$ where $n_{k+1}>n_k$ such that $d(x_{n_{k+1}},x)<\frac{1}{k+1}.$ Then $x_{n_k}\ra x$ as $k\ra \infty$ so $X$ is sequentially compact.
\end{proof}
\begin{lemma}
Let $(X,d)$ be a metric space and $A\subset X$ a closed subset. Then $A$ is compact.
\end{lemma}
\begin{proof}
Let $A\subset \cup_\alpha O_\alpha$ for $O_\alpha$ open. Then $X=(\cup_\alpha O_\alpha)\cup (X-A)$. Finitely many of these sets cover $X$ so finitely many of $\cup_\alpha O_\alpha$ cover $A$.
\end{proof}
\begin{theorem}
Let $[a,b]\subset \R$ be compact with $a\leq x\leq b$. Then the set of $[a_1,b_1]\times [a_2,b_2]\subset \R^2$ is compact.
\end{theorem}
\begin{corollary}
Let $A\subset \R^n$ be a set that's closed and bounded. Then $A$ is compact.
\end{corollary}
\begin{definition}[Continuous Functions Revised]
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces. Then $f:X\ra Y$ is continuous at $x_0$ if for all $\eps$ there exists $\delta>0$ such that $d(x,x_0)<\delta\implies d_Y(f(x),f(x_0))<\eps$.
\end{definition}
We will talk about everything below and including theorem 80 in next class.
\begin{theorem}
$\sum_{n=1}^\infty a_n$ converges if and only if $\sum_{k=0}^\infty 2^ka_{2^k}$ converges.
\end{theorem}
\begin{theorem}
If $\sum a_n$ is a series of complex numbers that converges absolutely, then every rearrangement of $\sum a_n$ converges and then all ocnverge to the same sum. Conversely, if the series converges but not absolutely, then given $-\infty\leq\alpha\leq\beta\leq\infty$ there exists a rearrangement of the series such that $$\liminf_{n\ra\infty} \sum a_n=\alpha\quad \textrm{and} \quad \limsup_{n\ra\infty}\sum a_n=\beta$$
\end{theorem}
\section{October 15, 2020}
\begin{lemma}
$[a_1,b_1]\times[a_2,b_2]$ is compact.
\end{lemma}
\begin{lemma}
A closed and bounded set $A$ of $\R^n$ with the Euclidean metric is compact.
\end{lemma}
\begin{proof}
Let $A\subset\cup O_\alpha$ with $O_\alpha$ open. Then $X=A\cup (X-A)\subset \cup O_\alpha\cup (X-A)$ Since $X$ is comapact, $X\subset O_1\cup...\cup O_n\cup (X-A)$ so $A\subset O_1\cup...\cup O_n$.
\end{proof}
\begin{theorem}
Consider $\R^n$ with the Euclidean metric. A subset $A\subset \R^n$ is compact if and only if it's closed and bounded.
\end{theorem}
\begin{proof}
If $A$ is bounded, $A\subset[-R,R]\times...\times[-R,R]$ for $R$ sufficiently large. This is a compact set. If $A$ is also closed then it's compact from the previous lemma.
\end{proof}
\begin{definition}[Continuity for arbitrary metric spaces]
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces and $f:X\ra Y$. We say $f$ is continuous at $x_0$ if given $\eps>0$ there exists $\delta>0$ such that $d_X(x_0,x)<\delta\implies d_Y(f(x_0),f(x))$.
\end{definition}
\begin{lemma}
The composition of two continuous functions is continuous.
\end{lemma}
\begin{proof}
Given $\eps>0$ there exists $\delta>0$ such that $d_Y(f(x_0),f(x))<\delta\implies d_Z(g(f(x_0),g(f(x)))<\eps$. There exists $\gamma$ such that $d_X(x,x_0)<\gamma\implies d_Y(f(x),f(x_0))<\delta$.
\end{proof}
\begin{definition}[The Derivative]
Let $f:I\ra\R$, $x_0\in I$ we say that $f$ is differentiable with derivative $a$ if for all $\eps>0$ there exists $\delta>0$ such that $$|x-x_0|<\delta\implies\left|\frac{f(x)-f(x_0)}{x-x_0}-a\right|<\eps$$
\end{definition}
\begin{example}
Suppose $f(x)=c$ for all $x$. Then $\frac{f(x)-f(x_0)}{x-x_0}=0$   so the derivative of this function is zero.\\
Let $f(x)=x$. Then the derivative is $$\lim_{x\ra x_0}\frac{x-x_0}{x-x_0}=1$$
\end{example}
\begin{lemma}
Let $f$ and $g$ be differentiable functions. Then $(f+g)'=f'+g'$, $(fg)'=f'g+g'f$, $(f(g(x))'=f'(g(x))g'(x)$.
\end{lemma}
\begin{example}
All polynomials are differentiable. Look at $x^n=x\cdot x\cdot...\cdot x$. Using the Leibniz rule gives $(x^n)'=nx^{n-1}$.
\end{example}
\begin{example}[Example of Continuity]
We want to show $f(x)=\frac{1}{x}$ is a continuous function. We show that if $x_0\in(0,\infty)$ then $f$ is continuous at $x_0$. Let $\eps>0$. We want to find $\delta>0$  such that $|x-x_0|<\delta\implies |\frac{1}{x}-\frac{1}{x_0}|<\eps$, or equivalently $|x-x_0|<\eps xx_0$. There is no $\delta>0$ for which $|x-x_0|<\delta\implies |x-x_0|<\eps xx_0$ for all $x\in(0,\infty)$. We can remove this problem by requiring $x$ to stay away from zero. For example, let $|x-x_0|<\frac{1}{2}x_0$. Then $\frac{1}{2}x_0<x$ and $\frac{1}{2}\eps x_0^2<\eps xx_0$. These inequalities suggest taking $$\delta=\min\left(\frac{1}{2}x_0,\frac{1}{2}x_0^2\eps\right).$$ For this $\delta$, if $|x-x_0|<\delta$ then $\frac{1}{x}-\frac{1}{x_0}|=\frac{|x-x_0|}{|xx_0|}<\frac{\frac{1}{2}x_0^2\eps}{\frac{1}{2}x_0^2}=\eps$.
\end{example}
\section{October 27, 2020}
\begin{lemma}
If $f(x)$ is differentiable then it is continuous.
\end{lemma}
\begin{proof}
If $\left|\frac{f(x)-f(x_0)}{x-x_0}-f'(x_0)\right|<\eps$ then $|f(x)-f(x_0)-f'(x_0)(x-x_0)|<\eps |x-x_0|$. Thus $|f(x)-f(x_0)|<(|f'(x_0)|+1)|x-x_0|$ when $|x-x_0|<\mu$. To show that $f$ is continuous
\end{proof}
\begin{proposition}
The derivative of the sum of two functions is the sum of the derivatives of the functions
\end{proposition}
\begin{proposition}
$(fg)'=f'g+fg'$
\end{proposition}
\begin{proof}
$\frac{f(x)g(x)-f(x_0)g(x_0)}{x-x_0}= \frac{g(x)(f(x)-f(x_0))}{x-x_0}+f(x_0)\frac{g(x)-g(x_0)}{x-x_0}$
\end{proof}
Do chain and quotient rules yourself. We did Rolle's Theorem. Mean Value Theorem. Do the product rule too.
\begin{theorem}[Mean Value Theorem]
If $f$ is a continuous real function on $[a,b]$ which is differentiable in $(a,b)$ then there is a point $x\in(a,b)$ at which $f(b)-f(a)=(b-a)f'(x)$.
\end{theorem}
The proof of this will follow as the corollary the the following lemma:
\begin{lemma}
If $f$ and $g$ are continuous real functions on $[a,b]$ which are differentiable in $(a,b)$ then there is a point $x\in(a,b)$ at which $[f(b)-f(a)]g'(x)=[g(b)-g(a)]f'(x)$. Note that differentiability is not required at the endpoits.
\end{lemma}
\begin{proof}
Put $h(t)=[f(b)-f(a)]g(t)-[g(b)-g(a)]f(t)$. Then $h$ is continuous on $[a,b]$, $h$ is differentiable on $(a,b)$ and $h(a)=f(b)g(a)-f(a)g(b)=h(b)$. To prove this theorem, we have to show that $h'(x)=0$ for some $x\in (a,b)$. If $h$ is constant, this holds for every $x\in(a,b)$. If $h(t)>h(a)$ for some $t\in(a,b)$, let $x$ be a point on $[a,b]$ at which $h$ attains its maximum. Thus $h'(x)=0$. If $h(t)<h(a)$ for some $t\in(a,b)$ the same argument applies of we choose for $x$ a point on $[a,b]$ where $h$ attains its minimum.
\end{proof}
\section{October 29, 2020}
Last time we showed what it means to be differentiable for a function. If $f(x)$ is differentiable on $(a,b)$ then it is continuous on $(a,b)$. We showed Rolle's Theorem. If $f(a)=f(b)$ for some $a\neq b$ then there exists $a<c<b$ such that $f'(c)=0$. We showed the Mean Value Theorem. If $f:[a,b]\ra\R$ is differentiable, $\frac{f(b)-f(a)}{b-a}=f'(c)$ for some $c\in(a,b)$. Note that if $f(a)=f(b)$ then $MVT$ is equivalent to Rolle's Theorem. Today, we'll talk about the L'Hospital's Law and the Taylor Expansion.
\begin{proposition}
Let $g,f$ be differential functions on $[a,b]$. Then $\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(c)}{g'(c)}$.
\end{proposition}
\begin{proof}
By the Mean Value Theorem, $\frac{f(b)-f(a)}{b-a}=f'(c)$ and $\frac{g(b)-g(a)}{b-a}=g'(\tilde{c})$. Then $\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(c)}{g'(\tilde{c})}$. We want to show that $c=\tilde{c}$. Look at $h(x)=(f(b)-f(a))g(x)-(g(b)-g(a))f(x)$. Then $h(a)=(f(b)-f(a))g(a)-(g(b)-g(a))f(a)=f(b)g(a)-g(b)f(a)$. And $h(b)=g(a)f(b)-g(b)f(a)$ so $h(a)=h(b)$. Then there exists $c$ such that $h'(c)=0$ so $0=h'(c)=(f(b)-f(a))g'(c)-(g(b)-g(a))f'(c)\implies \frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(c)}{g'(c)}$. We assumed that $f$ and $g$ are differentiable and that $f'$ and $g'$ are continuous.
\end{proof}
\subsection{Taylor Expansion}
Let $f:[a,b]\ra\R$ where $f$ is differentiable and $f'$ is differentiable.
\begin{definition}[Taylor Polynomial]
Suppose $f$ is a real function on $[a,b]$ that exists for every $t\in(a,b)$. Let $\alpha$ and $\beta$ be distinct points of $[a,b]$ and define $$P(t)=\sum_{k=0}^{n-1}\frac{f^{(k)}(\alpha)}{k!}(t-\alpha)^k.$$ Then there exists $\alpha<x<\beta$ such that $f(\beta)=P(\beta)+\frac{f^{(n)}(x)}{n!}(\beta-\alpha)^n$.
\end{definition}
\begin{proof}
For $n=1$, this is just the mean value theorem. In general, the theorem shows that $f$ can be approximated by a polynomial of degree $n-1$ and that we can estimate the error if we know the bounds on $|f^{(n)}(x)|$.

Let $M$ be the number defined by $f(\beta)=P(\beta)+M(\beta+\alpha)^n$ and put $g(t)=f(t)-P(t)-M(t-\alpha)^n$, $a\leq t\leq b$. We have to show that $n!M=f^{(n)}(x)$ for some $x$ between $\alpha$ and $\beta$. Then $g^{(n)}=f^{(n)}-n!M$. Hence the proof will be complete if we can show that $g^{(n)}(x)=0$ for some $x$ between $\alpha$ and $\beta$. Since $P^{(k)}(\alpha)=f^{(k)}(\alpha)$ for $k=0,...,n-1$ we have $g(\alpha)=g'(\alpha)=...=g^{(n-1)}(\alpha)=0$. Our choice of $M$ shows that $g(\beta)=0$ so that $g'(x_1)=0$ for some $x_1$ between $\alpha$ and $\beta$ by the mean value theorem. Sine $g'(\alpha)=0$, we conclude similarly that $g''(x_2)=0$ for some $x_2$ between $\alpha$ and $x_1$. After $n$ steps, we arrive at the conclusion that $g^{(n)}(x_n)=0$ for some $x_n$ between $\alpha$ and $x_{n-1}$.
\end{proof}
\remark{Review what was gone over on October 27 and 29 again. To be honest, just go over the entirety of Chapter 5 from Rudin.}
\section{November 3, 2020}
A function $f$ is not always equal to its infinite Taylor expansion. It depends on whether the remainder term goes to zero.
\begin{example}
Let $f(x)=e^x$. Then $f'(x)=f(x)$ and $f(0)=1$. The Taylor series is $\sum_{i=0}^\infty\frac{x^i}{i!}$. The remainder term for $f(x)$ where $a=0$. $\frac{e^c}{k!}x^k=R_k(x)$ where $0<c<x$. Then $|R_k(x)|\leq \frac{e^xx^k}{k!}\ra 0$ as $k\ra \infty$. Not every function has a Taylor series expansion. The functions that are equal to their Taylor series are called \vocab{analytic}
\end{example}
\begin{example}
A function that is not equal to its Taylor series expansion. Let $f(x)=
\begin{cases}
0& x\leq 0\\
e^{\frac{1}{x^2}}&x>0
\end{cases}$
This function is infinitely differentiable everywhere except at zero and we can prove with a bit more work that $f^{(k)}(0)=0$ for all $k$. The Taylor series gives 0. This happens because the remainder term doesn't go to zero.
\end{example}
\begin{definition}[Riemann Integral]
Let $f:[a,b]\ra\R$ and $f(x)>0$. What is the area under the curve?
\end{definition}
Suppose we take a rectangle.
\begin{definition}[A Partition]
A partition on $[a,b]$ is a set of numbers with $a=x_0<x_1<...<x_{n-1}<x_n=b$.
\end{definition}
Define $L_p=\sum m_i(x_{i+1}-x_i)$ and $U_p=\sum M_i(x_{i+1}-x_i)$ and $\int_a^b f=\sup_P L_p=\inf_P U_p$ where the $\sup$ is taken over all partitions. We say a function $f$ is integrable if and only if $\sup_P L_p=\inf_P U_p$.

Which functions are integrable? All continuous functions are integrable. We prove this next time.

\section{November 5, 2020}
We went over some stuff about uniform continuity.
\begin{definition}
A function $f$ is uniformly continuous on an interval $I$ if given $\eps>0$ there exists $\delta>0$ such that for all $x\in I$, $|x-x'|<\delta\implies |f(x)-f(x')|<\eps$.
\end{definition}
\begin{theorem}
Let $f:[a,b]\ra\R$. If $f$ is constant, then $f$ is uniformly continuous.
\end{theorem}
\begin{proof}
We argue by contradiction. If not, then there exists an $\eps>0$ such that for all $\delta>0$ there exists $x_\delta,y_\delta$ such that $|x_\delta-y_\delta|<\delta$ and $|f(x_\delta)-f(y_\delta)|\geq\eps$. Let $\delta=2^{-n}$, $|x_n-y_n|<2^{-n}$ so $|f(x_n)-f(y_n)|\geq \eps$. But that interval is compact. There exists a subsequence $x_{n_k}$ that converges to $x$ and $y_{n_k}$ converges to $y$. Thus $f(x_{n_k})\ra f(x)$ and $f(y_{n_k})\ra f(y)$. There's a bit more to this proof that I didn't get.
\end{proof}
Next we want to use this to show that any continuous function is integrable.
\begin{theorem}
$f:[a,b]\ra\R$ constant. $f$ is integrable.
\end{theorem}
\begin{proof}
Fix $\eps_0=\frac{\eps}{b-a}$. Use that $f$ is equicontinuous to get a $\delta>0$ such that $|x-y|<\delta\implies |f(x)-f(y)|<\eps_0$. Let $P$ be a partition. $f:[x_i,x_{i+1}]\ra\R$. $$\inf_{[x,x_{i+1}]}f>\sup_{[x,x_{i+1}]} f-\eps_0.$$ This comes from the fact that the difference of the output of any two inputs that are $\delta$ way from each other is less than $\eps$. $$U(f,P)=\sum \sup_{[x_i,x_{i+1}]}f(x_{i+1}-x_i)<\sum \inf{[x_i,x_{i+1}]}f(x_{i+1}-x_i)$$

$f$ is continuous $\implies$ $f$ is uniformly continuous $\implies $ $f$ is integrable.
\end{proof}

Some more things: $\int_a^b(f+g)dx=\int_a^bfdx=\int_a^bgdx$. If $f\geq g$ for all $x$ then $\int_a^b f\geq\int_a^b g$.

\begin{theorem}[Fundamental Theorem of Calculus]
Suppose that $f$ is continuous and $F(x)=\int_a^xf$. Then $F'=f$.
\end{theorem}
\begin{proof}
$\frac{F(x)-F(x_0)}{x-x_0}\ra f(x_0)$ as $x\ra x_0$. $$F(x)=\int_a^xf=\int_a^{x_0}f+\int_{x_0}^xf.$$ $f:[a,x_0]\ra \R$, $f:[x_0,x]\ra\R$. Thus $$\frac{F(x)-F(x_0)}{x-x_0}=\frac{\int_{x_0}^xf}{x-x_0}.$$ $\inf_{[x_0,x]}f\leq \frac{F(x)-F(x_0)}{x-x_0}\leq \sup_{[x_0,x]}f$. As $x\ra x_0$, $\inf_{[x_0,x]}f\ra f(x_0)$ and $\sup_{[x_0,x]}f\ra f(x_0)$ so the desired claim follow.
\end{proof}
\begin{theorem}[Second Part of the Fundamental Theorem of Calculus]
$F:[a,b]\ra\R$. $F$ is differentiable and $F'=f$ is integrable. $F(b)-F(a)=
\int_a^bf$
\end{theorem}
\begin{proof}
Let $x_0<x_1<...<x_{n-1}$ be a partition of $[a,b]$. Then $F(b)-F(a)=\sum_{i=0}^{n-1}F(x_{i+1})-F(x_i)$. This proof isn't finished.
\end{proof}
\section{November 10, 2020}
We showed last time that continuous functions are integrable. This is also in Rudin. We also showed some things that facilitate calculating the integral. For example, $\sum_a^b(f+g)dx=\int_a^bfdx+\int_a^bgdx$. Also, $\int_a^b(cf)dx=c\int_a^bfdx$.

A crucial tool is the fundamental theorem of calculus. If $F:[a,b]\ra \R$ and $F$ is and $F'=f$ is integrable then $F(b)-F(a)=\int_a^b f(x)dx$.

The other version of the fundamental theorem of calculus says that if $f:[a,b]\ra\R$ is continuous and $F(x)$ is defined by $\int_a^xf(y)dy=F(x)$ then $F'(x)=f(x)$.
\begin{lemma}
Another neat trick is $\int_a^bf(x)dx=\int_a^cf(x)dx+\int_c^bf(x)dx$.
\end{lemma}
\begin{proof}
$P_1$ is a partition of $[a,c]$ and $P_2$ is a partition of $[c,b]$. Let $P=P_1\cup P_2$ $L(f,P_1)+L(f,P_2)=L(f,P)$. Likewise, $U(f,P)=U(f,P_1)+U(f,P_2)$. If $f$ is integrable on $[a,c]$ and on $[c,b]$ Find a partition $P_1$ such that $U(f,P_1)<L(f,P_1)+\eps/2$ and a partition $P_2$ such that $U(f,P_2)<L(f,P_2)+\eps/2$. Add them up and we get $U(f,P)-L(f,P)<\eps$.
\end{proof}
\subsection{Arc Length}
Suppose we have a curve in the plane. A \vocab{curve} is a map $\psi:I\ra \R^2$ such that $t\mapsto (f(t),g(t))$, $f$ and $g$ are continuously differentiable. Some topology shows that since $\pi_1(\psi)$ is continuously differentiable and $\pi_2(\psi)$ is continuously differentiable then $\psi$ is continuously differentiable.
\begin{definition}[Arc Length]
The arc length is defined to be $$\int_a^b \sqrt{(f')^2+(g')^2}dx$$
\end{definition}
\begin{definition}[Improper Integrals]
Let $f:[a,\infty)\ra\R$ with $f$ integrable. If $\lim_{x\ra\infty}\int_a^xf(y)dy$ exists then we say that $\int_a^\infty f(x)dx$ is an improper integral.
\end{definition}
\subsection{The functions sine and cosine}
Consider the map $s\mapsto (s,\sqrt{1-s^2})$. Then $f(s)=s$ and $g(s)=\sqrt{1-s^2}$, $g'(s)=\frac{-2s}{2\sqrt{1-s^2}}=-\frac{s}{\sqrt{1-s^2}}$. Then $$\int_{\cos\theta}^1\frac{1}{\sqrt{1-s^2}}ds=\theta$$ and
$$\int_0^{\sin\theta}\frac{1}{\sqrt{1-s^2}}ds=\theta.$$
We got these equations by calculating the arc length of a circle $\sqrt{1-x^2}=y$ between two points.
\section{November 12, 2020}
Now, we define the arcsin function which is the inverse function of $\sin$.

$$\arcsin(y)=\int_0^y\frac{1}{\sqrt{1-s^2}}ds$$

$$\arcsin(y)=\frac{\pi}{2}-\int_0^{\sqrt{1-y^2}}\frac{1}{\sqrt{1-s^2}}ds$$

Use this formula to investigate what happens when $y$ goes to $\pm 1$. Also use it to show that $\arcsin$ is continuous on $[-1,1]$.

Suppose $f(x)=\sum_{n=0}^\infty a_nx^n$ is differentiable. What is the derivative of $f(x)$? We would think that it's $f'(x)=\sum_{n=1}^\infty a_nnx^{n-1}$. How do we prove this?
\begin{definition}
Let $f_n$ be a sequence of function. $f_n\ra f$ pointwise if for all $x_0$ in $f_n(x_0)\ra f(x_0)$ for all $x_0$. "Convergence may be at different for each $x_0$.
\end{definition}
\begin{definition}
A sequence $f_n$ of functions converges uniformly to $f$ if and only if $$\max_{[a,b]}|f_n-f|\ra 0$$ as $n\ra\infty$. Equivalently, given $\eps>0$ there exists $N$ such that $m\geq N\implies |f_n(x)-f(x)|<\eps$
\end{definition}
\begin{lemma}
$f_n:[a,b]\ra\R$, $g:[a,b]\ra\R$. Assume $f_n\ra f$ pointwise and $f_n'\ra g$ uniformly. Then if $f'n$ is continuous then $g$ is continuous. Also, we claim that if $f$ is differentiable then $f'=g$.
\end{lemma}
\begin{proof}
$$\int_a^xf'_n(s)ds+f_n(a)=f_n(x)$$
$$\int_a^xg(s)ds+f(a)=G(x)$$
We claim that $f_n(x)\ra G(x)$ uniformly. $G$ is clearly differentiable with $G'=g$ and $f_n\ra f$ pointwise. Thus $f=G$. We then want to show that $f_n$ converes to $f$ uniformly.
$$f_n(x)=\int_a^bf_n'(s)ds+f_n(x)$$
$$|f_n(x)-G(x)|=|\int_a^x(f'(s)g(s))ds+f_n(a)-f(a)|\leq \int_a^x |f'(s)-g(s)|ds+(f_n(a)-f(a))$$
$$\leq \int_a^b|f_n'(s)-g(s)|ds+|f_n(a)-f(a)|\leq (b-a)\max_{[a,b]}|f'_n-g|$$
The first term goes to zero since $f_n\ra G$ uniformly and $|f_n(a)-g(a)|$ goes to zero as well for big enough $N$.
\end{proof}
\begin{definition}[Weierstrass M-test]
Let $f_n:I\ra\R$ be a sequence of functions with $|f_n|\leq M_n$ and $\sum M_n<\infty$. The norm of $f_n$ is defined to be $\max_{x\in I}|f_n(x)|$. Then $\sum_{k=0}^n f_k\ra \sum_{k=0}^\infty f_k$ uniformly.
\end{definition}
\begin{proof}
Let $n>m$. Then $|f_n-f_m|=\sum_{k=m+1}^n f_k$
\end{proof}
\begin{remark}
Just look the Weierstrass $M$ test in the textbook man I don't even know what this man is on about.
\end{remark}
\begin{lemma}
$\sum_{n=0}^\infty a_nx^n$ and $\lim_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|=\lambda$. If we let $R=\frac{1}{\lambda}$ We claim that $\sum_{k=0}^na_kx^k\ra\sum_{k=0}^\infty a_kx^k$ uniformly on $[-L,L]$ where $L<R$.
\end{lemma}
\section{November 17, 2020}
Uniform convergence is convenient for continuity and integrability.
$f_n:[a,b]\ra\R$ is continuous and $f_n\ra f$ uniformly then $f$ is  continuous and $\sum_a^b f_n ds\ra \int_a^b f ds$.
Let $f_n:[a,b]\ra\R$ be continuous and $f:[a,b]\ra \R$ and $f_n\ra f$ pointwise. Remember that this means that given $x_0\in [a,b]$ and $\eps$ there exists $N$ such that $n\geq N\implies |f_n(x_0)-f(x_0)|<\eps$. Uniform convergence means given $\eps$ there exists $N$ such that $n\geq N\implies |f_n(x)-f(x)|<\eps$ for all $x$.

We proved that if $f_n'\ra g$ uniformly and $f_n'$ is continuous then $f'$ is differentiable and $f'=\lim_{n\to\infty}f_n'$.
\begin{proposition}
If we have a power series $\sum a_kx^k$  and $\lim_{k\to\infty}|\frac{a_{k+1}}{a_k}|=\lambda$ and $R=\frac{1}{\lambda}$. Then $\sum a_k x^k$ converges uniformly on the domain $[-L,L]$ for all $L<R$ and it converges pointwise on the domain $(-R,R)$.
\end{proposition}

Note that there exist functions that are infinitely many times differentiable but is not given by a power series.
$$f(t) =
\begin{cases}
e^{-\frac{1}{t^2}} & t>0 \\
0 & t\leq 0
\end{cases}$$
Now let's go back to the contraction mapping theorem. Here is a reminder of the theorem. Let $(X,d)$ be a metric space and $T:X\ra X$ a function on the space. It is a contraction map if there exists $0<L<1$ for all $x_1,x_2\in X$ such that $d(T(x_1),T(x_2))<Ld(x_1,x_2)$. The theorem says that $T$ has a unique fixed point (a point $x$ such that $T(x)=x$).
We want to show the existence and uniqueness for a certain Ordinary Differential Equation.
An ODE is of the form:
Let $y:[a,b]\ra \R$ be differentiable.
$$
\begin{cases}
y'(x)=f(y(x))\\
y(a)=c
\end{cases}
$$
The first condition of the ODE implies that $y(x)=\int_a^x f(y(t)) dt+c$. $C([a,b])$ is the metric space consisting of continuous  functions with $$d(h_1,h_2)=\sup_{x\in[a,b]}{|h_1(x)-h_2(x)|}$$
Define $T(h)=c+\int_a^xf(h(s))ds$.
We want to say that $T$ is a contracting map. Once we know this is true we can show there is a unique $h$ such that $h=T(h)$.
\begin{proof}[Proof that T is a contraction map]
\begin{equation*}
    \begin{split}
        d(T(h_1),T(h_2)) & = \sup_{x\in[a,b]}|\int_a^xf(h(s))ds-\int_a^xf(h_2(s)ds|
    \end{split}
\end{equation*}
$|f(z_1)-f(z_2)|=|f'(z)(z_1-z_2)|\leq|z_1-z_2|$ where $z_1=h_1(s)$

Then $$|f(h_1(s))-f(h_2(s))|\leq L|h_1(s)-h_2(s)|$$

$$\sup_{x\in[a,b]}\int_a^b|f(h_1(s))-f(h_2(s))|ds$$

We get that the difference between these two functions is bounded by
$$\int_a^b|f(h_1(s))-f(h_2(s))|ds$$
We have a bound for the integrand though so this is less than or equal to $L\int_a^b|h_1(s)-h_2(s)|ds\leq L(b-a)d(h_1,h_2)$
\end{proof}
\section{November 19, 2020}
Last time we showed the existence and uniqueness of ODEs. The key point in the proof was to make the contracting maps theorem and FTC and MVT.
\begin{theorem}[Implicit Function Theorem]
Let $F:O\subseteq \R^2\ra\R$ where $F_x$ and $F_y$ are continuous and $F_y(0,0)\neq 0$. Then in a neighborhood of $(0,0)$ $y$ can be as a function $g(x)$ of $x$ and $g'(X)=-\frac{F_x}{F_y}$. The derivatives of $F$, $F_x$ and $F_y$, are continuous.
\end{theorem}
\begin{proof}
WLOG, let $F(0,0)=0$. After looking at a sufficiently small neighborhood of $x$, $\{(x,y):F(x,y)=0\}=\{(x,g(x)\}$.

It's enough to show that if $x_n\ra x$ then $g(x_n)\ra g(x)$ and $x_n\ra x$ nd $g(x_n)\ra y$ so $y=g(x)$. Enough to show $F(x_n,g(x_n))=0$ and $(x_n,g(x_n))\ra(x,y)$ and $F(x,y)=0$ so $g$ is continuous. We want to show that $g$ is differentiable. $F(x+h,g(x+h))-F(x,g(x))=G(1)-G(0)$. $G(s)=F(x+sh,g(x)+s(g(x+h)-g(x)))$. This is just a function of one variable. $G'=F_xh+F_y(g(x+h)-g(x))$. $0=F_xh+F_u(g(x+h)-g(x))$

$\frac{g(x+h)-g(x)}{h}=-\frac{F_x(x,g(x))}{F_y(x,g(x))}$
\end{proof}
\section{December 1, 2020}
To take a \vocab{partial derivative} with respect to $x_1$ of a function $f:x_1,x_2,...,x_n\mapsto f(x_1,x_2,...,x_n)$, fix $x_2,x_3,...,x_n$ then take the derivative with respect to $x_1$.
\begin{definition}[Directional Derivatives]
$h(s)=f(x_0+su,y_0+sv)$
\end{definition}
Let $f:D\subseteq\R^2\ra\R$ with second derivatives all existing.
\section{December 3, 2020}
Today is kind of boring.
\begin{theorem}
If $f$ as above has $\frac{\partial^2 f}{\partial x\partial y}$ and $\frac{\partial^2 f}{\partial y\partial x}$ continuous. Then the order in which we differentiate doesn't matter.
\end{theorem}
\section{Analysis Final Review}
Just some stuff to review.
\begin{theorem}[Taylor's Theorem]
  Suppose we have a function $f$ whose $n$th derivative exists on $[a,b]$. Suppose we want to get an estimate of $f$ at $x\in[a,b]$ and we know $f^{(k)}(\alpha)$ for all $k\leq n$. Then, there exists $c\in(\alpha,x)$ such that
  $$f(x)=\sum_{k=0}^{n-1}\frac{f^{(k)}(\alpha)}{k!}(x-\alpha)^k+\frac{f^{(n)}(c)}{n!}(x-\alpha)^n$$
\end{theorem}
\begin{tabular}{|c|c|}
\hline
Comparison & Suppose $0\leq a_n\leq b_n$ for all $n$. If $b_n$ converges then $a_n$ converges. \\
\hline
Ratio & If $\limsup_{n\ra\infty}|\frac{a_{n+1}}{a_n}|<1$ then the sum converges.\\
\hline
Root & If $\limsup_{n\ra\infty}\sqrt[n]{|a_n|}<1$ then the sum converges.\\
\hline
Alternating Series Test & If $a_{n+1}a_n<0,|a_{n+1}|\leq|a_n|$, and $\lim_{n\ra\infty}a_n=0$ then the sum converges. \\
\hline
\end{tabular}
\begin{theorem}[Implicit Function Theorem]
Let $F:O\subseteq \R^2\ra\R$ where $F_x$ and $F_y$ are continuous and $F_y(0,0)\neq 0$. Then in a neighborhood $U$ of $(0,0)$, $y$ can be found as a function $g(x)$ of $x$. What this means is that there exists a function $g:U\ra \R$ that satisfies $f(x,g(x))=0$. Also, $g'(X)=-\frac{F_x}{F_y}$.
\end{theorem}
Note that $F_x$ means the partial derivative of $F$ with respect to $x$ and $F_y$ means the partial derivative of $F$ with respect to $y$. There are also some other bizarre tests for convergence such as
\begin{theorem}[Cauchy Condensation Test]
  The series $\sum_{n=0}^\infty f(n)$ converges if and only if $\sum_{n=0}^\infty 2^nf(2^n)$ converges.
\end{theorem}
\begin{theorem}
  Given $\sum b_n$ and $\sum a_n$. Suppose $\lim_{n\to\infty}\frac{a_n}{b_n}>0$ then $\sum a_n$ converges if and only if $\sum b_n$ converges.
\end{theorem}
\begin{proof}
  Suppose $b_n$ diverges. Let $\lim_{n\to\infty}\frac{a_n}{b_n}=\lambda$. There exists $N$ such that $n\geq N\implies |\frac{a_n}{b_n}-\lambda|<\lambda/2$ or equivalently $\lambda/2<\frac{a_n}{b_n}<3\lambda/2$. Thus $b_n\lambda/2<a_n$. Finally,
  $$\sum_{n=0}^\infty a_n=\sum_{n=0}^{N-1}a_n+\sum_{n=N}^\infty a_n>\sum_{n=0}^{N-1}a_n+\sum_{n=N}^\infty b_n\lambda/2$$ which diverges. The case where $b_n$ converges is again similar.
\end{proof}
\begin{theorem}
  Suppose we have a sequence of functions $f_n:I\ra \R$ that satisfy some properties (differentiability, integrability, continuity, etc.) If $f_n$ converges uniformly then
    $$\lim_{n\to\infty}\frac{d}{dx}f_n(x) =\frac{d}{dx}\lim_{n\to\infty}f_n(x)$$
    $$\lim_{n\to\infty}\int_a^b f_n(x)dx = \int_a^b\lim_{n\to\infty}f_n(x)dx$$
and $\lim_{n\to\infty}f_n(x)$ is continuous on $I$.
\end{theorem}
Some Analysis definitions.
\begin{definition}
  The limit of $a_n$: $a_n$ converges to $L$ if and only if given $\eps>0$ there exists $N$ such that $n\geq N\implies |a_n-L|<\eps$
\end{definition}
\begin{definition}
  Continuity of $f$ at $x_0$: $f$ is continuous at $x_0$ if and only if given $\eps>0$ there exists $\delta$ such that $|x-x_0|<\delta\implies |f(x)-f(x_0)|<\eps$. Limits respect continuity. $\lim_{x\to x_0}f(x)=f(x_0)$.
\end{definition}
\begin{definition}
  Uniform convergence on interval $I$ The sequence $f_n$ converges to $f$ uniformly if and only given $\eps$ there exists $N$ such that $n\geq N\implies |f_n(x)-f(x)|<\eps$ for all $x\in I$.
\end{definition}
Some Topology Definitions
\begin{definition}
  A set $A$ is open if and only if for all $x\in A$ there exists $r\in \R$ such that $B_r(x)\subseteq A$. Infinite union and finite intersection.
\end{definition}
\begin{definition}
  A set $A$ is closed if and only if $X-A$ is open. Equivalently, $A$ is closed if and only if for all sequences $x_n:\N\ra A$, $\lim_{n\to\infty}x_n\in A$. Infinite intersection and finite union.
\end{definition}
\begin{definition}
  A set $X$ is compact if and only if for all open covers $X\subseteq \cup_{n=0}^\infty A_n$ there exists $\{A_{n_i}:0\leq i \leq N\}$ such that $X\subseteq \cup_{i=0}^NA_{n_i}$
\end{definition}
Some reflections from the homework.
\begin{fact}
  $\sum_{n=1}^\infty\frac{1}{n}$ is a good counterexample for some things.
\end{fact}
\begin{problem}
  Let $C(I)$ be the set of continuous functions on a closed interval $I$. Suppose we want to show that $C(I)$ is Cauchy complete with metric
  $$d(f,g)=\sup_{x\in I}|f(x)-g(x)|$$
\end{problem}
\begin{proof}
  First we show that if $f_n$ converges then it's Cauchy.\\
  Given $\eps$ there exists $N$ such that $n,m\geq N\implies d(f_n,f_m)<\eps$. There exists $N$ such that $n,m\geq N\implies d(f_n,f)<\eps/2$ and $d(f_m,f)<\eps/2$. Then $d(f_n,f_m)\leq d(f_n,f)+d(f_m,f)<\eps$. Conversely, suppose for all $\eps$ there exists $N$ such that $n,m\geq N\implies |f_n(x)-f_m(x)|<\eps/2$ for all $x\in I$.
  Then since $f_m(x)$ converges to some function $g(x)$ as $n\ra\infty$ since $\R$ is Cauchy complete there exists $M$ such that $m\geq M\implies |g(x)-f_m(x)|<\eps/2$. Thus $|f_n(x)-g(x)|\leq|f_n(x)-f_m(x)|+|f_m(x)-g(x)|<\eps$ for all $x\in I$.
\end{proof}
\begin{theorem}
  Suppose we have a series $\sum a_n$ with $\limsup_{n\to\infty}|\frac{a_{n+1}}{a_n}|=\lambda$ and $R=\frac{1}{\lambda}$. Then $\sum a_n$ converges uniformly on $[-L,L]$ where $L<R$ and converges piecewise on $[-R,R]$.
\end{theorem}
\begin{theorem}[The Main Value theorem]
  Let $f:[a,b]\ra \R$ be differentiable on $(a,b)$. Then there exists $c\in(a,b)$ such that $f'(c)=\frac{f(b)-f(a)}{b-a}$.
\end{theorem}
\begin{problem}
  Suppose $f(x)$ is differentiable on $(-\infty,0)\cup(0,\infty)$ and $\lim_{x\to 0}f'(x)$ exists.
\end{problem}
\begin{proof}
  Given $\eps$ there exists $\delta$ such that $|x|<\delta\implies |f'(x)-L|<\eps$. There exists $c\in(0,x)$ such that $f'(c)=\frac{f(x)-f(0)}{x-0}$. Since $|c|<\delta$, $|f'(c)-L|<\eps$ so $|\frac{f(x)-f(0)}{x-0}-L|<\eps$
\end{proof}
Some stuff with ODEs:
\begin{definition}
  An ODE is of the form
  \begin{equation*}
    \begin{cases}
      y'(x)=f(y(x)) \\
      y(a)=c
    \end{cases}
  \end{equation*}
\end{definition}
We can show that every ODE has a solution. Let's first make sure some conditions are met. $f$ must be differentiable and its differentiable must be bounded by $L$.
\begin{proof}
  These conditions, coupled with the fundamental theorem of calculus, says that
  $$y(x)=\int_a^xf(y(x))dx+c$$
  Suppose we're working over the space $I=[a,b]$ with metric
  $$d(f,g)=\sup_{x\in I}|f(x)-g(x)|.$$
  Define the functional $T:C(I)\ra C(I)$ by $T(h(x))=\int_a^xf(h(s))ds+c$. We now have to show that $T$ is a contraction mapping. This is equivalent to saying that $d(T(\alpha(x)),T(\beta(x)))\leq \lambda d(\alpha(x),\beta(x))$ for $\lambda<1$. A useful assumption to have will be that $L(b-a)<1$. First, observe that
  $$|f(\alpha(x))-f(\beta(x))|=|f'(c)(\alpha(x)-\beta(x))|\leq L|\alpha(x)-\beta(x)|$$
  for some $c\in (\alpha(x),\beta(x))$.
  \begin{equation*}
    \begin{split}
      d(T(\alpha),T(\beta)) & =\sup_{x\in I}|\int_a^x(f(\alpha(s))-f(\beta(s)))ds| \\
      & \leq \sup_{x\in I}\int_a^x|f(\alpha(s))-f(\beta(s))|ds \\ %triangle inequality
      & \leq L\sup_{x\in I}\int_a^x|\alpha(s)-\beta(s)|ds \\ %pull out the L
      & = L\int_a^b|\alpha(s)-\beta(s)|ds \\
      & \leq L\sup_{s\in I}(b-a)|\alpha(s)-\beta(s)| \\
      & = L(b-a)d(\alpha(x),\beta(x))
    \end{split}
  \end{equation*}
\end{proof}
If instead we were working over $[A,B]$ and $L(B-A)\geq 1$. Then we can cover $[A,B]$ with closed intervals of length $b-a$. We can guarantee that there is a unique solution for the ODE on each of the subintervals. Thus furnishes a function that satisfies the ODE on the entire interval $[A,B]$.

For one of the homework problems, there was the issue that the derivative of $f$ was not bounded above by $L<1$. We instead showed the uniqueness of a solution and not its existence (though it wouldn't be hard to show). We showed that if $y_1(x_0)=y_2(x_0)$ then $y_1(x)=y_2(x)$ for all $x\in [x_0,x_0+b]$.
\begin{problem}
  Let $y:[0,1]\ra \R$. Show that the solution to the following ODE is unique.
  \begin{equation*}
    \begin{cases}
      y'(x) = y^2(X) \\
      y(0) = a
    \end{cases}
  \end{equation*}
\end{problem}
\begin{proof}
  Suppose $y_1(x_0)=y_2(x_0)$. Let $M=\sup_{x\in I}|y_1(x)+y_2(x)|$. Then for all $x\in[x_0,x_0+b]$.
  \begin{equation*}
    \begin{split}
      |y_1(x)-y_2(x)| & = |\int_a^xy_1^2(x)-y_2^2(x)dx| \\
      & \leq \int_{x_0}^x | y_1^2(x)-y_2^2(x)|dx \\
      & \leq \int_{x_0}^x |y_1(x)+y_2(x)||y_1(x)-y_2(x)|dx \\
      & \leq M\int_{x_0}^x |y_1(x)-y_2(x)|dx \\
      & \leq Mb\sup_{x\in[x_0,x_0+b]}|y_1(x)-y_2(x)|
    \end{split}
  \end{equation*}
  We can set $b$ arbitrarily so let it be $\frac{1}{2M}$. But then this implies that $y_1(x)=y_2(x)$ for all $x\in[x_0,x_0+b]$. We know from the ODE condition that $y_1(0)=y_2(0)$. But then $y_1(x)=y_2(x)$ for all $x\in[0,b]$. Do this trick again setting $x_0=b$. We get that $y_1(x)=y_2(x)$ for all $x\in[0,2b]$.
  We can do this inductively to get that $y_1(x)=y_2(x)$ for all $x\in[0,1]$.
\end{proof}
\begin{problem}
  Suppose given $\eps$ there exists $N$ such that $n,m\geq N\implies |f_n(x)-f_m(x)|<\eps/2$ for all $x\in I$ and given $x$ and $\eps$ there exists $M$ such that $m\geq M\implies |f_m(x)-f(x)|<\eps/2$. Then I claim that for $n\geq N |f_n(x)-f(x)|<\eps$.
\end{problem}
\begin{proof}[Solution to Problem 141]
  Note that $N$ doesn't depend on $x$ but $M$ does. Then $|f_n(x)-f(x)|\leq |f_n(x)-f_m(x)|+|f_m(x)-f(x)|<\eps$ for all $n\geq N$ and $x$.
\end{proof}
\begin{problem}
  Let $F$ be a subset of $\R$ and $E=\{-|x|:x\in F\}$. Express $\inf E$ in terms of $\inf F$.
\end{problem}
\begin{proof}[Solution to Problem 142]
  I claim that this is $\inf E = \min\{\inf F,-\sup F\}$. Let's say $F$ is both bounded and is nonempty. We know that for $x\in F$, $\inf F\leq x\leq \sup F$. If $x\geq 0,-|x|\geq -\sup F$ and if $x<0,-|x|\geq \inf F$. Thus $-|x|$ is always bounded below by either $\inf F$ or $-\sup F$. Hence $\inf E\geq \min\{\inf F,-\sup F\}$.
  Next, $\inf E\leq -|x|\leq x\leq |x|\leq -\inf E$. This shows that $\inf E\leq \inf F$ and $\inf E\leq -\sup F$ so $\inf E\leq \min\{\inf F,-\sup F\}$. Thus $\inf E=\min\{\inf F,-\sup F\}$.
\end{proof}
\begin{problem}
  We know that $$e^x=\sum_{n=0}^\infty\frac{x^k}{k!}.$$ Let's play around with it.
\end{problem}
\begin{proof}[Playing around with e]
  The \vocab{Weierstrass M-Test} says that if there exists $M_n$ such that $M_n\geq |\frac{x^k}{k!}|$ for all $x\in[-L,L]$ and $\sum_{n=0}^\infty M_n$ converges then $\sum_{n=0}^\infty \frac{x^k}{k!}$ converges uniformly on $[-L,L]$. Let $x\in[-L,L]$.
  Look at $|\frac{x^k}{k!}|\leq \frac{L^k}{k!}$. We can show using, say, the ratio test, that $\sum_{n=0}^\infty\frac{L^k}{k!}$ converges. Then $\sum_{n=0}^\infty \frac{x^k}{k!}$ converges uniformly on $[-L,L]$. The weierstrass $M$-test is actually pretty easy to show.\\
  Now let's use the \vocab{Taylor Series Expansion} of $e^x$. Suppose we know that $\frac{d}{dx}e^x|_{x=0}=1$. Taylor's theorem says that
  $$e^x=\sum_{n=0}^{N-1}\frac{x^k}{k!}+\frac{e^{c_N}}{N!}x^N$$ for some $c_N\in(0,x)$. By the extremal value theorem, $f$ has a maximal value $M$ on $[0,x]$. Now, for a fixed $x$, the limit as $N$ approaches infinity of $\frac{Mx^N}{N!}$ is 0 so we can be confident that $e^x=\sum_{n=0}^\infty \frac{x^n}{n!}$.
\end{proof}
\begin{problem}
  Let $f_n$ be a sequence of Lipschitz functions each with the same Lipschitz constant $L$. Show that if $f_n\ra f$ pointwise on $[a,b]$ then it converges uniformly.
\end{problem}
\begin{proof}
  It's easy to show that $f$ is Lipschitz with Lipschitz constant $L$ so let's just assume it. Given $\eps>0$. Create a partition $x_0,x_1,...,x_m$ of $[a,b]$ such that $x_{i+1}-x_i<\frac{\eps}{3L}$ for all $i$. For each of these $x_i$, there exists $N_i$ such that $n\geq N_i\implies |f_n(x_i)-f(x_i)|<\eps/3$. Let $N=\max_iN_i$. Then,
  \begin{equation*}
    \begin{split}
      |f_n(x)-f(x)| & \leq |f_n(x)-f_n(x_i)+f_n(x_i)-f(x_i)+f(x_i)-f(x)| \\
      & \leq |f_n(x_i)-f_n(x_i)|+|f_n(x_i)-f(x_i)|+|f(x_i)-f(x)| \\
      & < L\frac{\eps}{3L}+\frac{\eps}{3}+L\frac{\eps}{3L}
    \end{split}
  \end{equation*}
\end{proof}
Now for the stuff that wasn't in psets/I wasn't paying attention to when it was taught.
\begin{definition}[The Directional Derivative]
  Let $\vec u =<a,b>$ be a unit vector. Then the directional derivative
  $$
  D_uf(x,y)=\lim_{h\to0}\frac{f(x+ah,y+bh)-f(x,y)}{h}
  $$
\end{definition}
\begin{theorem}
  Let $f$ be continuously differentiable at a point $(x_0,y_0)$. Then
  $$
  \frac{\partial f}{\partial x\partial y}=\frac{\partial f}{\partial x\partial y}
  $$
\end{theorem}
\begin{problem}
  Let $(X,d)$ be a compact metric space and $A_j$ a sequence of closed sets. Prove that if
  $$
  X\cap(\bigcap_{n=0}^\infty A_j)=\emptyset
  $$
  then
  $$
  X\cap(\bigcap_{n=0}^N A_j)=\emptyset
  $$
  for some finite $N$
\end{problem}
\begin{proof}
  Let $B_j=X-A_j$. $B_j$ are all open sets. Then if $X\cap(\bigcap_{n=0}^\infty A_j)=\emptyset$ then
  $$
  \bigcup_{n=0}^\infty B_j=X
  $$
  Since $X$ is compact, there are finitely many $B_j$ that cover $X$. This proves the claim.
\end{proof}
\begin{problem}
  Consider the metric space $(C([0,1]),d)$ where $C([0,1])$ denotes the set of continuous functions on $[0,1]$ and
  $$
  d(f,g)=\sup_{x\in[0,1]}|f(x)-g(x)|
  $$
  Is the set $S=\{f(x)\in C([0,1]):f(x)\geq 0\forall x\in[0,1]\}$ open, closed, or neither?
\end{problem}
\begin{proof}
  $S$ is open if and only if for all $f\in S$, there exists $r$ such that $f\in B_r(f)\subseteq S$. Remember that
  $$
  B_r(f)=\{g\in C([0,1]):\sup_{x\in[0,1]}|f(x)-g(x)|<r\}
  $$
  Now, it's clear that $S$ can't be open because $f(x)=0\in S$ but for all $r$, $g(x)=-\frac{r}{2}\notin S$ but $|f(x)-g(x)|=\frac{r}{2}<r$. To show it's closed, let's see if the complement $T$ is open.
  $$
  T=\{f\in C([0,1]):\exists x\in[0,1] s.t. f(x)<0\}
  $$
  Let $f\in T$ and $r=-\inf_{x\in[0,1]}f(x)$ and $f(x_0)=-r$. Then $r>0$ and for all $g\in B_\frac{r}{2}(f)$, $|g(x_0)-f(x_0)|\leq\sup_{x\in[0,1]|f(x)-g(x)|}<\frac{r}{2}$ so $g(x_0)<-\frac{r}{2}<0$.
  Thus $S$ is closed and $T$ is open.
\end{proof}
\begin{definition}
  Given $f$, and a partition $P$ with $x_0,x_1,...,x_m$, define
  $$
  m_i=\inf_{x\in[x_{i+1},x_i]}f(x)\quad M_i=\sup_{x\in[x_{i+1},x_i]}f(x)
  $$
  and
  $$
  L_p=\sum m_i(x_{i+1}-x_i)\quad U_p=\sum M_i(x_{i+1}-x_i)
  $$
  We say $f$ is integrable if and only if
  $$
  \sup_P L_p=\inf_P U_p
  $$
  Observe that integrability is defined on an interval like $[a,b]$. This is unlike continuity or differentiability that are defined at a single point $x_0$.
\end{definition}
\begin{theorem}[Calculus]
  Here are some theorems related to calculus. Let $f:[a,b]\ra \R$ and $F(x)=\int_a^xf(s)ds$.
  \begin{itemize}
    \item Suppose $f$ is continuous. Then $f$ is integrable.
    \item Suppose $f$ is integrable on $[a,b]$. Then $F$ is continuous on $[a,b]$.
    \item Suppose $f$ is continuous at $x_0$. Then $F$ is differentiable at $x_0$.
    \item Suppose $F'=f$. Then $\int_a^bf(s)ds=F(b)-F(a)$. We have a verify that $F$ is differentiable and that $f$ is integrable. We get both of these things for free if we just show that $f$ is continuous.
  \end{itemize}
\end{theorem}
\begin{theorem}[L'Hopital's Rule]
  Suppose $f$ and $g$ are differentiable on a deleted neighborhood $N$ of $a$ and satisfy the following properties:
  \begin{itemize}
    \item $\lim_{x\to a}f(x)=0$
    \item $\lim_{x\to a}f(x)=0$
    \item $\lim_{x\to a}\frac{f'(x)}{g'(x)}$ exists
    \item For every $x\in N$, $g'(x)\neq 0$.
  \end{itemize}
\end{theorem}
\begin{proof}
  We didn't require that $f$ and $g$ be defined at $a$ so let $f(a)=g(a)=0$ to get a continuous function. (By the generalized mean inequality, there exists $c_x\in (a,x)$ such that
  $$
  \frac{f(x)-f(a)}{g(x)-g(a)}=\frac{f'(c_x)}{g'(c_x)}\implies \frac{f(x)}{g(x)}=\frac{f'(c_x)}{g'(c_x)}
  $$
  We are justified in expressing this as a fraction because $g(x)\neq 0$ for $x\in N\cap\{x:x>a\}$. If for some $x>a$, we had $g(x)=0$, Rolle's theorem would furnish a point $z$ such that $g'(z)=0$, contradicting the fourth constraint. A similar argument shows that $g(x)\neq 0$ for all $x\in N$. Now,
  $$
  \lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(c_x)}{g'(c_x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)}
  $$
\end{proof}
The case for $\lim_{x\to\infty}$ is easy if we set $f(x)=F(-\frac{1}{x})$ and $g(x)=G(-\frac{1}{})$.
\begin{theorem}[Cauchy's Generalized Mean Theorem]
  Suppose $f$ and $g$ are continuous on $[a,b]$ and are differentiable on $(a,b)$. Then there exists $c\in (a,b)$ such that
  $$
  \frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(c)}{g'(c)}.
  $$
\end{theorem}
\begin{problem}
  Show that $f(x)=\frac{1}{x}$ is continuous on $\R-\{0\}$.
\end{problem}
\begin{proof}
  By the definition of continuity, $f(x)$ is continuous at $x_0$ if and only if given $\eps>0$ there exists $\delta$ such that $|\frac{1}{x}-\frac{1}{x_0}|<\eps$. Let
  $$
  \delta = \min\{\eps,\frac{1}{2}|x_0|\}
  $$
  Then $|x_0|-|x|\leq |x-x_0|<\frac{1}{2}|x_0|\implies |x|>\frac{1}{2}|x_0|$
  \begin{equation*}
    \begin{split}
      |\frac{1}{x}-\frac{1}{x_0}| & = |\frac{x_0-x}{xx_0}|\\
      & < \frac{\eps}{|x||x_0|} \\
      & < \frac{\eps}{\frac{1}{2}|x_0|}
    \end{split}
  \end{equation*}
\end{proof}
\end{document}
